\documentclass[10pt,a4paper]{article}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{array}
\usepackage{booktabs}

\usepackage{multirow}
\usepackage{booktabs} % for extra vertical spacing in tables


\usepackage{url}
\usepackage{fancyvrb}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\usepackage{breakurl}

\newcommand{\UBL}{\texttt{UBL}\ }

\newcommand{\pUBL}{package \texttt{UBL}\ }
\newcommand{\UBLp}{\texttt{UBL}\ package  }
\newcommand{\version}{0.0.7}

\author{Paula Branco, Rita P. Ribeiro and Luis Torgo\\FCUP - LIAAD/INESC Tec\\University of Porto\\
  \texttt{ \{paula.branco,rpribeiro,ltorgo\}@dcc.fc.up.pt}}
\title{UBL: an R Package for Utility-Based Learning}

\begin{document}


<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
## set global chunk options
opts_chunk$set(cache.path="cache/UBL-",dev="pdf",fig.path="figures/UBL-",
               fig.align="center", fig.show="hold",tidy=FALSE,size="footnotesize",message=FALSE, cache = TRUE)
options(replace.assign=TRUE,width=90)
##
##library(devtools)
##load_all("~/Software/R/MyPackages/UBLn")
library(UBL)
library(ggplot2)
library(reshape2)
library(xtable)
library(gridExtra)
library(plotrix)
library(performanceEstimation)
@
%$
\maketitle


\begin{abstract}
  
  This document describes the R \pUBL that allows the use of several methods for handling utility-based learning problems. Classification and regression problems that assume non-uniform costs and/or benefits pose serious challenges to predictive analytic tasks. In the context of meteorology, finance, medicine, ecology, among many other, specific domain information concerning the preference bias of the users must be taken into account to enhance the models predictive performance. To deal with this problem, a large number of techniques was proposed by the research community for both classification and regression tasks. 
  The main goal of \UBLp is to facilitate the utility-based predictive analytic task by providing a set of methods to deal with this type of problems in the R environment. It is a versatile tool that provides mechanisms to handle both regression and classification (binary and multiclass) tasks. Moreover, \UBLp allows the user to specify his domain preferences, but it also provides some automatic methods that try to infer those preference bias from the domain, considering some common known settings. 

\end{abstract}

% ====================================================================
\section{Introduction}

This document describes the methods available in \pUBL \footnote{This document was written for \UBLp version \version.} to deal with utility-based problems. \UBLp aims at providing a diverse set of methods to address predictive tasks where the user has a non-uniform preference bias across the domain. The package provides tools suitable for both classification and regression tasks. All the methods available in \UBLp were extended for being able to deal with multiclass problems and with regression problems possibly containing several relevant regions across the target variable domain. 


Utility-based problems are defined in the context of predictive tasks where the user has a differentiated interest over the domain. This means that, in this type of problems, the user has non-uniform benefits for the correct predictions and/or assumes non-uniform costs for different errors. Many real world applications are utility-based learning problems because they encompass domain specific information which, if disregarded, may strongly penalize the performance of predictive models. This happens in the context of meteorology, finance, medicine, ecology, among many other, where specific domain information concerning the user preferences must be taken into account to enhance the models predictive performance.


In the utility-based learning framework we can frequently witness the conjugation of two important factors: i) an increased interest in some particular range(s)/class(es) of the target variable values and ii) a scarce representation of the examples belonging to that range(s)/class(es). This situation occurs in both classification and regression tasks and is usually known as the problem of imbalanced domains \cite{branco2015survey}.

Utility-based learning assumes non-uniform costs and/or benefits which are usually expressed through a cost/benefit matrix (in classification) or a cost/benefit surface (in regression). However, frequently this information is just not available, or is hard/expensive to obtain because it often requires the intervention of a domain expert. This means that for many domains, there is only an informal knowledge regarding which are the most costly mistakes and which are the most important classes/ranges of the target variable. In fact, considering the particular problem of imbalanced classes it is frequent to observe the assumption that ``the minority class is the most important one". This is an important information regarding the preferences of the user. However, it is stated in a very informal way, an no cost/benefit matrix is available in this situation. The approaches proposed in \pUBL are able to deal with these situations because they allow the use of both user specified preferences and automatic methods.



% The methods implemented in \UBL are pre-processing approaches which aim at altering the original data set to match the user preferences. This means that the methods for dealing with imbalanced domains are only relevant when the user preferences are focused on the least represented examples. In fact, if the most frequent cases are the most important, then the learning algorithms will naturally tend to focus on these examples, and therefore there is no need to change the original distribution. On the other hand, when the most relevant examples are scarcely represented any learning algorithm used will focus on the normal cases and will fail the most important predictions on the rare examples.

Several types of approaches exist for handling utility-based learning problems. These approaches were categorized into: pre-processing, change the learning algorithms, post-processing or hybrid \cite{branco2015survey}. The \textbf{pre-processing} approaches act before the learning stage by manipulating the examples distribution to match the user preferences. The methods that \textbf{change the learning algorithms} try to incorporate the user preference bias into the selected learning algorithm. There are also strategies that are applied as a \textbf{post-processing} step by changing the predictions made by a standard learner using the original data set. Finally there are \textbf{hybrid} approaches that combine some of the previous strategies.


In \pUBL we have focused on pre-processing strategies to address the problem of utility-based learning. These strategies change the original distribution of examples by removing or/and adding examples, i.e., by performing under-sampling or over-sampling. The under-sampling strategies may be random or focused. By focused under-sampling we mean that the discarded examples satisfy a given requirement, such as: are possibly noisy examples, are too distant from the decision border, are too close to the border, etc. Regarding the over-sampling methods there are two main options: over-sampling is accomplished by the introduction of replicas of examples or by the generation of new synthetic examples. For the strategies which include copies of existing examples, the cases may be selected randomly or in an informed fashion. Approaches that build synthetic cases differ among themselves in the generation process adopted. Several strategies combine under-sampling and over-sampling methods in different ranges/classes of the target variable.

This document is organized as follows. In Section \ref{sec:instal} some general installation guidelines for \UBLp are provided. Section \ref{sec:syntdata} briefly describes the two synthetic data sets provided with \UBLp. Section \ref{sec:example} presents two simple examples to show how \UBLp can be used both in classification and regression contexts and its impact on the models performance. Sections \ref{sec:methClass} and \ref{sec:methRegres} describe with detail each method currently implemented in \UBL for classification and regression tasks. Section \ref{sec:distFunc} describes the distance functions available in \pUBL which allow to asses the distance between examples in data sets containing nominal and/or numeric features. Finally, Section \ref{sec:conc} concludes this document.

% ====================================================================
\section{Package Installation Guidelines}\label{sec:instal}

The installation of any R package available on CRAN is performed as follows:
<<message=FALSE, eval=FALSE>>=
install.packages("UBL")
@

This is mandatory, if you want to use the approaches available in \pUBL or even if you just want to try out the examples presented in the next sections. This installs the current stable version of \texttt{UBL} package which is version \version.

You may also install the development version of the package, that is available on the following GitHub Web page: \url{https://github.com/paobranco/UBL}. However, we strongly recommend the use of the CRAN stable version. If you still want to install the development version, you should do this with extreme care because this version is still being tested and therefore is more prone to bugs.
To install the development version from GitHub you should do the following in R:

<<message=FALSE, eval=FALSE>>=
library(devtools)
install_github("paobranco/UBL",ref="development")
@

Further instructions may be found at the mentioned GitHub page. 
For reporting issues related with \UBLp you can use: \url{https://github.com/paobranco/UBL/issues}.


After installation using any of the above procedures, the package can be used as any other R package by doing:

<<message=FALSE, eval=FALSE>>=
library(UBL)
@

%Further help and illustrations can be obtained through the many help pages of each function defined in the package that contain lots of illustrative examples. Again, these help pages can be accessed as any other R package, through R help system (e.g. running \texttt{help.start()} at R command line). If you just want to run some examples of a particular function you can simply use \texttt{example(<function name>)}.

\section{Synthetic Data for Classification and Regression Tasks}\label{sec:syntdata}


Package \texttt{UBL} includes two artificially generated data sets: one for classification (ImbC) and another for regression (ImbR). Both data sets were generated to depict situations of imbalanced domains. Thus, in both data sets, we assume the usual setting where the under-represented values of the target variable (either nominal or numeric) are the most important to the user. Table~\ref{tab:data} summarizes the main characteristics of these data sets.

\begin{table}[!hbt]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccccccc}
    \toprule
    \multirow{2}{*}{Dataset}
    & \multirow{2}{*}{Task} &
    Total 
    & 
    \multicolumn{2}{c}{Relevant}
    & \multicolumn{5}{c}{Features}&\multirow{2}{*}{Target}\\
    \cline{6-10}
    &&Cases&\multicolumn{2}{c}{Cases}&name&type&min&mean&max\\
    \midrule
    \multirow{2}{*}{ImbC} & \multirow{2}{*}{Classif.} &\multirow{2}{*}{1000} &rare1&rare2&      X1&num.&-13.58 &-0.11&12.78& minority: rare1; rare2\\   
&&&10&131&X2&nom. &"cat" 300 &"fish" 300&"dog 400"& majority: normal\\
\midrule
    \multirow{3}{*}{ImbR} & \multirow{3}{*}{Regress.} &\multirow{3}{*}{1000} & \multicolumn{2}{c}{\multirow{3}{*}{50}}&X1&num.&0.37 &9.94&19.06&min: 10.00 \\
    &&&&&X2&num.&0.20 &10.08&19.47& mean: 10.98\\
    &&&&&&&&&&max: 23.17 \\
    \bottomrule     
\end{tabular}
}
\caption{Description of artificial data sets of \UBLp.}
\label{tab:data}

\end{table}

ImbC data consists of multiclass classification data set with 1000 cases and two features, $X1$ (numeric) and $X2$ (nominal). The target variable, denoted as $Class$, has two minority classes (\textit{rare1} and \textit{rare2}) and a majority class (\textit{normal}). The percentage of cases of classes \textit{rare1} and \textit{rare2} is 1\% and 13.1\%, respectively, while the \textit{normal} class has 85.9\% of the cases. This data set mimics the usual setting where the most relevant classes for the user are under-represented. This data set also simulates the existence of both class overlap and small disjuncts. Both issues are known for increasing the difficulty of dealing with imbalanced domains~\cite{lopez2013insight}. Figure~\ref{fig:OriginalC} shows this data set with some noise added to the nominal variable $X2$ to make the examples distribution more visible.


ImbC data set was generated as follows:
\begin{itemize}
  \item $X1 \sim \mathbf{N} \left(0, 4\right)$
  \item $X2$ labels "cat", "fish" and "dog" where randomly distributed with the restriction of having a   frequency of 30\%, 30\% and 40\% respectively.
  \item To obtain the target variable $Class$, we have define the following sets:
  \begin{itemize}
    \item $S_1=\{(X1, X2) : X1 > 9 \wedge (X2 \in \{"cat", "dog"\})\}$
    \item $S_2=\{(X1, X2) : X1 > 7 \wedge X2 = "fish" \}$
    \item $S_3=\{(X1, X2) :-1  <  X1 < 0.5\}$
    \item $S_4=\{(X1, X2) : X1 < -7 \wedge X2 = "fish"\}$
  \end{itemize}
  \item The following conditions define the target variable distribution of the ImbC synthetic data set:
  \begin{itemize}
    \item Assign class label "rare1" to: a random sample of 90\% of set $S_1$ and a random sample of 40\% of set $S_2$
    \item Assign class label "rare2" to: a random sample of 80\% of set $S_3$ and a random sample of 70\% of set $S_4$
    \item Assign class label "normal" to the remaing examples.
  \end{itemize}
\end{itemize}


<<OriginalC, fig.cap="ImbC: artificial data set for classification.",out.width="0.5\\textwidth", echo=FALSE>>=
data(ImbC)

ggplot(ImbC, aes(x = X2, y = X1)) + geom_jitter(data = ImbC, aes(colour=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) 

@


<<OriginalR, fig.cap="ImbR: artificial data set for regression.",out.width="0.5\\textwidth", echo=FALSE>>=
data(ImbR)

ggplot(ImbR, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) + geom_point(data = ImbR, aes(colour=Tgt)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5))

@
The regression data set ImbR has two numeric features ($X1$ and $X2$) and a continuous target variable $Tgt$. ImbR was generated in the following way: it includes 50 cases sampled from a circumference with white noise and the remaining 950 cases were sampled from a two dimensional normal distribution. Regarding the values of the continuous target variable ($Tgt$), they were obtained through a sample of two different Gamma distributions (one with higher values used for generating the target variable values of the examples in the circumference, and another with lower values used for the target variable values of the remaining examples). ImbR data simulates the usual setting in regression where the most relevant values are under-represented. In this case, we consider that the higher values of the target variable, whose predictor variables where sampled from a cirfumference, are the most important ones.

More formally, ImbR data was obtained as follows:
\begin{itemize}
  \item lower $Tgt$ values:
  \begin{itemize}
    \item $(X1, X2) \sim \mathbf{N}_{2} (\mathbf{10}_{2}, \mathbf{2.5}_{2})$
    \item $Tgt \sim \mathbf{\Gamma} \left( 0.5, 1 \right) + 10$
  \end{itemize}
  \item higher $Tgt$ values: 
  \begin{itemize} 
    \item $(X1, X2) \sim \left(\rho * cos(\theta) + 10, \rho * sin(\theta) + 10 \right)$, where $\rho \sim \mathbf{9}_{2}+\mathbf{N}_{2} \left(\mathbf{0}_{2}, \mathbf{I}_{2} \right)$ and $\theta \sim \mathbf{U}_{2} \left( \mathbf{0}_{2}, 2\pi \mathbf{I}_{2} \right)$ 
    \item $Tgt \sim \mathbf{\Gamma} \left( 1,1 \right) + 20$
    \end{itemize}
\end{itemize}


Figure~\ref{fig:OriginalR} shows the examples distribution of ImbR data set.
% ====================================================================
\section{Two Simple Illustrative Examples}\label{sec:example}

%Let us consider a classification task with 3 classes with different frequency. For illustration purposes, we will use the well-known iris data set with some examples removed to simulate a distribution with a rare class. Let us also suppose that the most important class for the user is this rare class (in our example, the virginica class). 
In this section we will show two simple examples of how to use the \UBLp. We will use the two data sets provided with the package (ImbC and ImbR) to illustrate a classification and a regression task.


Consider the ImbC synthetic data set. Let us suppose that the most important classes are the two minority classes, rare1 and rare2, and that we do not consider class normal relevant. Assuming this domain information, we begin by briefly observing the data set characteristics.

<<ImbC_data>>=
library(UBL)     # Loading our infra-structure
library(e1071)   # packge containing the svm we will use
data(ImbC)       # The synthetic data set we are going to use
summary(ImbC)    # Summary of the ImbC data  
table(ImbC$Class) 
@ 


Now we will obtain a random sample of 70\% of our data to train a svm.
Then, we observe the results on the remaining 30\% of data left for testing. We obtain the following:

<<ImbC_svm_unbalanced>>=
set.seed(123)
samp <- sample(1:nrow(ImbC), nrow(ImbC)*0.7)
train <- ImbC[samp,]
test <- ImbC[-samp,]

model <- svm(Class~., train)
preds <- predict(model,test)
table(preds, test$Class) # confusion matrix 
@

Clearly, the model presents a poor performance on least represented classes. In effect, in this case, the model always predicts class normal.



Now, we can try to apply a pre-processing strategy for dealing with utility-based problems, and check again the models performance. In this case we selected the common strategy of balancing the data set classes and we used the SMOTE algorithm proposed by \cite{CBOK02}.


<<ImbC_svm_unbalanced2>>=
# change the train data by applying the smote strategy
# notice that we have to set the dist parameter to for instance "HEOM" 
# because the default distance (Euclidean) is not possible to use with nominal features
newtrain <- SmoteClassif(Class~., train, C.perc="balance", dist="HEOM") 


# generate a new model with the changed data
newmodel <- svm(Class~., newtrain)
preds <- predict(newmodel,test)
table(preds, test$Class)

@

We can observe that the least represented classes, rare1 and rare2, now present an improved result.
If the previous model was unable to correctly classify any examples of these classes, now most of those cases have a correct prediction. However, it is also important to highlight the increase in misclassification of class normal.


We can also observe the results obtained by applying a simple random over-sampling method. Again, we opted to balance the problem classes.

<<ImbC_svm_unbalanced3>>=
# apply random over-sampling strategy
newtrain2 <- RandOverClassif(Class~., train, C.perc="balance")

#generate a new model with the modified data set
newmodel2 <- svm(Class~., newtrain2)
preds <- predict(newmodel2, test)
table(preds, test$Class)

@

Again, the pre-processing method applied allowed to improve the performance of the model on the least represented (and more important) class.



Let us now see how the pre-processing strategies can be applied on a regression task using the ImbR synthetic data set.

We start by loading the data and observe its main characteristics. Let us consider a random sample of 70\% of ImbR data for training a model. The remaining 30\% will be used as test set.

<<ImbR_samp>>=
library(UBL)
data(ImbR)
summary(ImbR)

set.seed(123)
samp <- sample(1:nrow(ImbR), as.integer(0.7*nrow(ImbR)))
trainD <- ImbR[samp,]
testD <- ImbR[-samp,]
@


It is required that the user states which are the cases that he considers more/less relevant. The \UBLp includes an automatic method for defining a relevance of the examples based on the data distribution. This method, proposed by \cite{ribeiro2011utility}, allows to obtain a relevance function that maps each target variabel value into a $[0,1]$ scale of relevance, where 1 represents maximum relevance and 0 represent minimum relevance. The key aspect of this automatic method is the assignment of an higher relevance to the scarcely represented cases which is the most common setting. We will use this automatic method in the ollowing example. A more detail explanation of this method is provided in Section~\ref{sec:methRegres}.


Let us train a model using the original train data. We choose a random forest available through the \texttt{randomForest} package and obtained the predictions on the test set.

<<ImbR_RF>>=
library(randomForest)
model <- randomForest(Tgt~., trainD)
preds <- predict(model, testD)

@

Let us now apply a pre-processing strategy in the original train data and observe the impact on the predictions.

<<ImbR_RF_gn>>=
# use the Introduction of Gaussian Noise with the default parameters
newTrain <- GaussNoiseRegress(Tgt~., trainD) 
newModel <-randomForest(Tgt~., newTrain)
newPreds <- predict(newModel, testD)

@

The results obtained by the two random forest models are displayed in Figure~\ref{fig:ImbR_RF1}. In this case, it is clear that the predictions obtained by the model trained with the changed data set are better in the high rare cases. For the higher range of values of $Tgt$ the model trained with the original data displays mostly under-predictions showing a focus in the normal range of values.

<<ImbR_RF1, fig.cap="Predictions obtained with the original and the new data modified through the Gaussian Noise strategy.",out.width="0.7\\textwidth", echo=FALSE, fig.height=7, fig.width=8>>=
dsO <- data.frame(testD, preds=preds)
dsC <- data.frame(testD, preds=newPreds)

pDS <- rbind(dsO,dsC)

pDS$model <- c(rep("Original data", nrow(testD)), rep("Changed data", nrow(testD)))

ggplot(data=pDS, aes(x=Tgt, y=preds, col=model, pch=model)) + geom_point() +
  geom_abline(slope=1, intercept=0, color="grey", linetype=2)

@


We can also observe the use impact of using the simple random under-sampling pre-processing strategy.

<<ImbR_RU>>=
# random under-sampling strategy setting the under-sampling percentage to 0.3
trainRU <-RandUnderRegress(Tgt~., trainD, C.perc=list(0.3)) 
ModelRU <-randomForest(Tgt~., trainRU)
PredsRU <- predict(ModelRU, testD)
@


<<ImbR_RU1, fig.cap="Predictions obtained with the original and the new data modified through the random under-sampling strategy.",out.width="0.7\\textwidth", echo=FALSE, fig.width=8, fig.height=7>>=
dsO <- data.frame(testD, preds=preds)
dsC <- data.frame(testD, preds=PredsRU)

pDS <- rbind(dsO, dsC)

pDS$model <- c(rep("Original data", nrow(testD)), rep("Changed data", nrow(testD)))

ggplot(data=pDS, aes(x=Tgt, y=preds, col=model, pch=model)) + geom_point() +
  geom_abline(slope=1, intercept=0, color="grey", linetype=2)

@



Figure~\ref{fig:ImbR_RU1} shows the predictions obtained with the original training set and the training data modified through random under-sampling strategy.


A more complex pre-processing can also be tried. In this case, we apply smoteR algorithm with low percentage of over-sampling. Then, we add more synthetic examples using the Gaussian Noise strategy using only the cases with a relevance value above the 0.8 threshold. Figure~\ref{fig:ImbR_Comb1} shows the predictions obtained with these changes.

<<ImbR_Comb>>=
train1 <- SmoteRegress(Tgt~., trainD, C.perc=list(0.9, 2))
train2 <- GaussNoiseRegress(Tgt~., train1, thr.rel=0.8, C.perc=list(0.8, 2), pert=0.01)
ModelC <-randomForest(Tgt~., train2)
PredsC <- predict(ModelC, testD)
@


<<ImbR_Comb1, fig.cap="Predictions obtained with the original and the new data modified through the combination of strategies.",out.width="0.7\\textwidth", echo=FALSE, fig.height=7, fig.width=8>>=
dsO <- data.frame(testD, preds=preds)
dsComb <- data.frame(testD, preds=PredsC)

pDS <- rbind(dsO, dsComb)

pDS$model <- c(rep("Original data", nrow(testD)), rep("Changed data", nrow(testD)))

ggplot(data=pDS, aes(x=Tgt, y=preds, col=model, pch=model)) + geom_point() +
  geom_abline(slope=1, intercept=0, color="grey", linetype=2)

@



Figure~\ref{fig:ImbR_difs} shows each test set example marked by a point with size and color varying according to the error magnitude for all the models previously obtained. This means that larger blue points represent larger errors and small green points represent a lower magnitude of the error in the example. 

<<ImbR_difs,fig.cap="Predictions obtained with the original data and the two training sets modified through random under-sampling and Gaussian Noise strategies.",out.width="0.9\\textwidth", echo=FALSE>>=
dsGN <- data.frame(testD, preds=newPreds)
dsRU <- data.frame(testD, preds=PredsRU)
dsComb <- data.frame(testD, preds=PredsC)
pDS <- rbind(dsO, dsGN, dsRU, dsComb)

pDS$model <- c(rep("Original", nrow(testD)), rep("Gauss Noise", nrow(testD)), rep("Random Under-samp", nrow(testD)), rep("Strategies Combined", nrow(testD)))

pDS$dif <- NULL
pDS$dif <- abs(pDS$Tgt-pDS$preds)

g1 <- ggplot(data=pDS[pDS$model=="Original",], aes(x=X1, y=X2)) +
  geom_point(aes(size = dif, colour=dif)) +
  scale_colour_gradient(low="green", high="blue") +
  scale_size(range=c(0.1,4)) +
  guides(color=guide_legend(title="error"), size = guide_legend(title="error")) +
  ggtitle("Original data")

g2 <- ggplot(data=pDS[pDS$model=="Gauss Noise",], aes(x=X1, y=X2)) + 
  geom_point(aes(size = dif, colour=dif)) + 
  scale_colour_gradient(low="green", high="blue") +
  scale_size(range=c(0.1,4)) +
  guides(color=guide_legend(title="error"), size = guide_legend(title="error")) +
  ggtitle("Use of Gaussian Noise")


g3 <- ggplot(data=pDS[pDS$model=="Random Under-samp",], aes(x=X1, y=X2)) + 
    geom_point(aes(size = dif, colour=dif)) + 
  scale_colour_gradient(low="green", high="blue") +
  scale_size(range=c(0.1,4)) +
  guides(color=guide_legend(title="error"), size = guide_legend(title="error")) +
  ggtitle("Use of Random Under-sampling")

g4 <- ggplot(data=pDS[pDS$model=="Strategies Combined",], aes(x=X1, y=X2)) + 
    geom_point(aes(size = dif, colour=dif)) + 
  scale_colour_gradient(low="green", high="blue") +
  scale_size(range=c(0.1,4)) +
  guides(color=guide_legend(title="error"), size = guide_legend(title="error")) +
  ggtitle("Combination of strategies")

plots <- list(g1, g2, g3, g4, ncol=2)

do.call(grid.arrange, plots)

@

% ====================================================================
\section{Methods for Addressing Utility-based Classification Tasks }\label{sec:methClass}

In this section we describe the methods implemented in \pUBL. We provide detailed examples of each function, and discuss how the several parameters can be used and their impact.
The methods explained in this section are the following:
\begin{itemize}
\item \ref{sec:RUClassif}: Random Under-sampling
\item \ref{sec:ROClassif}: Random Over-sampling
\item \ref{sec:ISClassif}: Importance Sampling
\item \ref{sec:Tomek}: Tomek Links
\item \ref{sec:CNN}: Condensed Nearest Neighbors
\item \ref{sec:OSS}: One-sided Selection
\item \ref{sec:ENN}: Edited Nearest Neighbors
\item \ref{sec:NCL}: Neighborhood Cleaning Rule
\item \ref{sec:gnClassif}: Gaussian Noise Introduction
\item \ref{sec:smoteClassif}: Smote Algorithm
\item \ref{sec:adasynClassif}: Adasyn Algorithm
\end{itemize}


\subsection{Random Under-sampling}\label{sec:RUClassif}

The random under-sampling strategy is among the simplest strategies for dealing with the class imbalanced problem. To force the learners to focus on the most important and least represented class(es) this technique randomly removes examples from the most represented and less important classes. This process allows to obtain a more balanced data set, although some important data may have been discarded with this technique. Another side effect of this strategy is a big reduction on the number of examples in the data set which facilitates the learners task although some important data may be ignored.

This strategy is implemented in \UBL taking into consideration the possible existence of several minority classes. The user may define through \texttt{C.perc} parameter which are the normal and less important classes and the under-sampling percentages to apply in each one of them. Another possibility is to select ``balance" or ``extreme" for the parameter \texttt{C.perc}. These two options automatically estimate the under-sampling percentages to apply to the classes. The ``balance" option obtains a balanced number of examples in all the existing classes, and the ``extreme" option inverts the existing frequencies, transforming the most frequent classes into the less frequent and vice-versa. The following examples show how these options can be used and their impact.

<<iris_ru>>=
library(UBL)  # Loading our infra-structure
library(e1071) # package containing the svm we will use
data(ImbC)                      # Our synthetic multiclass data set

table(ImbC$Class)

## now, using random under-sampling to create a
## "balanced problem" automatically

newData <- RandUnderClassif(Class ~ ., ImbC)
table(newData$Class)
@

We highlight that, because this method only allows the removal of cases, in order to balance the examples distribution (the function default that was used), it has a strong impact in the total number of examples in the changed data set. This happens because one  of the minority classes has only 10 examples. Figure \ref{fig:Iris_RU1} shows the impact of this strategy in the examples distribution.


<<Iris_RU1,fig.cap="The impact of random under-sampling strategy.",out.width="0.8\\textwidth", echo=FALSE, fig.height=10, fig.width=7>>=
#   par(mfrow = c(1, 2))
#   plot(ImbC[, 2], ImbC[, 1], pch = as.integer(ImbC[, 3]), col=as.integer(ImbC[,3]),
#        main = "Original Data")
#   plot(newData[, 2], newData[, 1], pch = as.integer(newData[,3]), col=as.integer(newData[,3]),
#        main = "Under-sampled Data")

g1 <- ggplot(ImbC, aes(x = X2, y = X1)) + geom_jitter(data = ImbC, aes(colour=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) + ggtitle("Original Data")

g2 <- ggplot(newData, aes(x = X2, y = X1)) + geom_jitter(data = newData, aes(colour=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) + ggtitle("Under-sampled Data")

plots <- list(g1, g2)

do.call(grid.arrange, plots)

@ 

Another example with ImbC data set:

<<another_iris_exRU>>=
  RUmy <- RandUnderClassif(Class~., ImbC, list(normal=0.1, rare2=0.9))
  RUB <- RandUnderClassif(Class~., ImbC, "balance")
  RUE <- RandUnderClassif(Class~., ImbC, "extreme")

@


<<RU_tab, echo=FALSE, results='asis'>>=
nm <- c("Original","RUmy", "RUB","RUE")

res <- c(table(ImbC$Class),table(RUmy$Class), table(RUB$Class), table(RUE$Class))

m <- matrix(res, nrow=4,ncol=3, byrow=TRUE, dimnames=list(nm, names(table(ImbC$Class))))

xt <-xtable(m, caption="Number of examples in each class for different parameters of random under-sampling strategy.",label="tab:RU_tab")
print(xt,comment = FALSE, type = 'latex')

@

The impact of the strategies on the number of examples in each class of the data set are in Figure\ref{fig:Iris_RU2}.



<<Iris_RU2, fig.cap="Random Under-sampling strategy for different parameters values.", echo=FALSE,fig.height=10>>=
DF <- rbind(data.frame(Dat="Original",classes=ImbC$Class), data.frame(Dat="RUmy", classes=RUmy$Class), data.frame(Dat="RUB", classes=RUB$Class), data.frame(Dat="RUE", classes=RUE$Class))

g1 <- ggplot(DF,aes(x=classes, fill=Dat, colour=Dat))+geom_bar( position="dodge", aes(group=Dat), colour="black")+xlab("ImbC classes")

g2 <- ggplot(DF,aes(x=Dat, fill=classes, colour=classes))+geom_bar( position="fill", aes(group=classes), colour="black")+xlab("Original and pre-processed data sets")

plots <- list(g1, g2)

do.call(grid.arrange, plots)
@



\subsection{Random Over-sampling}\label{sec:ROClassif}

The random over-sampling strategy introduces replicas of already existing examples in the data set. The replicas to include are randomly selected among the least populated and more important classes. This allows to obtain a better balanced data set without discarding any examples. However, this method has a strong impact on the number of examples of the new data set which can represent a difficulty to the used learner.

This strategy is implemented in \pUBL taking into consideration the possible existence of several minority classes. The user may define through \texttt{C.perc} parameter which are the most important classes and their respective over-sampling percentages. The parameter \texttt{C.perc} may also be set to ``balance" or ``extreme". These two options automatically estimate the classes and over-sampling percentages to apply. Similarly to the previous strategy the ``balance" option allows to obtain a balanced number of examples in all the existing classes, and the ``extreme" option inverts the existing frequencies, transforming the most frequent classes into the less frequent and vice-versa. The following examples show how these options can be used and their impact:

<<iris_RO>>=
## now using random over-sampling to create a 
## data with more 500% of examples in the 
## rare1 class
RO.U1<- RandOverClassif(Class ~ ., ImbC, 
                        C.perc=list(rare1=5))
RO.U2<- RandOverClassif(Class ~ ., ImbC, 
                        C.perc=list(rare1=4, rare2=2.5))
RO.B <- RandOverClassif(Class ~ ., ImbC, C.perc="balance")
RO.E <- RandOverClassif(Class ~ ., ImbC, C.perc="extreme")
@



<<RO_tab, echo=FALSE, results='asis'>>=

nm <- c("Original","RO.U1", "RO.U2", "RO.B","RO.E")

res <- c(table(ImbC$Class),table(RO.U1$Class), table(RO.U2$Class), table(RO.B$Class), table(RO.E$Class))

m <- matrix(res, nrow=5,ncol=3, byrow=TRUE, dimnames=list(nm, names(table(ImbC$Class))))


xt <-xtable(m, caption="Number of examples in each class for different Random over-sampling parameters.",label="tab:RO_tab")
print(xt,comment = FALSE, type = 'latex')

@

Figure \ref{fig:IrisRO} shows the impact of this strategy in the examples distribution. We have introduced a small perturbation on the examples position to be more clear the replicas that were introduced.

<<IrisRO,fig.cap="The impact of random over-sampling Strategy.",out.width="0.8\\textwidth", echo=FALSE>>=
g1 <- ggplot(ImbC, aes(x = X2, y = X1)) + geom_jitter(data = ImbC, aes(colour=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) + ggtitle("Original ImbC data")

g2 <- ggplot(RO.U1, aes(x = X2, y = X1)) + geom_jitter(data = RO.U1, aes(colour=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) + ggtitle("RO.U1 data")

g3 <- ggplot(RO.B, aes(x = X2, y = X1)) + geom_jitter(data = RO.B, aes(colour=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) + ggtitle("RO.B balanced data")

g4 <- ggplot(RO.E, aes(x = X2, y = X1)) + geom_jitter(data = RO.E, aes(colour=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) + ggtitle("RO.E data")

plots <- list(g1,g2,g3,g4, ncol=2)

do.call(grid.arrange, plots)
@ 



Figure \ref{fig:Iris_RO2} shows the impact of this strategy on the number of examples in the data set.

<<Iris_RO2, fig.cap="Impact of Random over-sampling strategy for different parameters values.", echo=FALSE>>=
DF <- rbind(data.frame(Dat="Original",obs=ImbC$Class), data.frame(Dat="RO.U1", obs=RO.U1$Class), data.frame(Dat="RO.B", obs=RO.B$Class), data.frame(Dat="RO.E", obs=RO.E$Class))
g1 <- ggplot(DF,aes(x=obs, fill=Dat, colour=Dat))+geom_bar(position="dodge", aes(group=Dat), colour="black")

g2 <- ggplot(DF,aes(x=Dat, fill=obs, colour=obs))+geom_bar( position="fill", aes(group=obs), colour="black")

plots <- list(g1, g2)

do.call(grid.arrange, plots)
@

\subsection{Importance Sampling}\label{sec:ISClassif}

The main idea of Importance Sampling strategy is to perform random over- or under-sampling in each class according to the importance assigned by the user. This means that for each class the user can specify its relevance. Then, this relevance is used to change each class frequency by selecting randomly examples from each class. Alternatively, the user may consider that each class is equally important or may chose to invert the classes frequencies.

This strategy is available in \UBLp through the function \texttt{ImpSampClassif}. The user may specify using parameter \texttt{C.perc} the classes where over-/under-sampling must be applied, by indicating the corresponding percentages. If all classes are equally important and a perfectly balanced data set should be obtained, the \texttt{C.perc} parameter must be set to ``balance". On the other hand, if the classes frequencies should be inverted, then this parameter should be ``extreme". The following example illustrate the use of this function.


<<ISClass>>=
# use the synthetic imbalanced data set ImbC provided with UBL package
table(ImbC$Class)

nds <- ImpSampClassif(Class~.,ImbC, C.perc=list(normal=0.4, rare1=6))
# notice that when a certain class is not specified it remains unaltered
table(nds$Class)

# to obtain a balanced data set
IS.bal <- ImpSampClassif(Class~., ImbC) # or use C.perc="balance"
table(IS.bal$Class)

# to obtain a data set with inverted frequencies 
IS.ext <- ImpSampClassif(Class~., ImbC, C.perc="extreme")
table(IS.ext$Class)

@



Figure \ref{fig:ISC} shows the impact on the imbalanced ImbC data set of the changes made in the domain with Importance Sampling.

<<ISC, fig.cap="Impact of Importance Sampling strategy in ImbC data set.", echo=FALSE>>=

g1 <- ggplot(ImbC, aes(x = X2, y = X1)) + geom_jitter(data = ImbC, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) + ggtitle("Original ImbC data")

g2 <- ggplot(nds, aes(x = X2, y = X1)) + geom_jitter(data = nds, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) + ggtitle("User defined parameters")

g3 <- ggplot(IS.bal, aes(x = X2, y = X1)) + geom_jitter(data = IS.bal, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) + ggtitle("Balanced data")
  
g4 <- ggplot(IS.ext, aes(x = X2, y = X1)) + geom_jitter(data = IS.ext, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) + ggtitle("\"Inverted\" data")

  
plots <- list(g1, g2, g3, g4, ncol=2)
do.call(grid.arrange, plots)

@



Figure \ref{fig:ISC_2} shows the impact of this strategy on the number of examples in the data set.

<<ISC_2, fig.cap="Impact of Importance Sampling strategy.", echo=FALSE>>=
DF <- rbind(data.frame(Dat="Original",obs=ImbC$Class), data.frame(Dat="nds", obs=nds$Class), data.frame(Dat="IS.bal", obs=IS.bal$Class), data.frame(Dat="IS.ext", obs=IS.ext$Class))
g1 <- ggplot(DF,aes(x=obs, fill=Dat, colour=Dat))+geom_bar(position="dodge", aes(group=Dat), colour="black")

g2 <- ggplot(DF,aes(x=Dat, fill=obs, colour=obs))+geom_bar( position="fill", aes(group=obs), colour="black")

plots <- list(g1, g2)

do.call(grid.arrange, plots)
@

We must highlight that random under- and over-sampling also allow to balance and invert the classes frequencies. Importance Sampling strategy, although also allowing this type of impact, acts differently because it combines both under- and over-sampling strategies. This means that a balanced data set can be obtained through random under-sampling, random over-sampling or importance sampling strategy. However, the resulting data sets will be different. If we use random under-sampling the final size of the data set is reduced, while if we use the random over-sampling approach the changed data set is significantly larger than the original one. If we select the importance sampling, the combination of the strategies allows to roughly maintain the data set size.

\subsection{Tomek Links}\label{sec:Tomek}

Tomek Links \cite{tomek1976two} can be defined as follows: two examples form a Tomek Link if and only if they belong to different classes and are each other nearest neighbors. This is a property existing between a pair of examples $(S_i, S_j)$ having different class labels and for which 

$\nexists S_k : dist(S_i,S_k) < dist(S_i,S_j) \vee dist(S_j, S_k)<dist(S_i,S_j)$

\noindent Having determined the examples which form Tomek Links, these connections may be explained because either the examples are both borderline examples or one of the examples may be considered as noise.
Therefore, there are two possibilities of using Tomek links to accomplish under-sampling:
\begin{itemize}
  \item remove the two examples forming a Tomek link, or
  \item only remove the example from the most populated class which forms a Tomek link.
\end{itemize}

These two options correspond to using Tomek link as cleaning technique (by removing both borderline examples) or as an under-sampling method for balancing the classes (by removing the majority class example).


In \pUBL we have adapted this technique for being able to deal with multiclass imbalanced problems. 
For working with more than two classes some issues were considered: 
\begin{itemize}
\item allow the user to select which classes should be under-sampled (if not defined, the default is to under-sample all the existing classes);
\item if the user selects a given number of classes what to do to break the link, i.e., how to decide which example(s) to remove (if any). 
\end{itemize}
So, in \UBL the user may chose from which classes he is interested in removing examples through the \texttt{Cl} parameter. Moreover, the user can also decide if both examples are removed or if just one is discarded using the \texttt{rem} parameter. If this can be easily understood in two class problems, the impact of these parameters may not be so clear for multiclass imbalanced tasks. 
In fact,the options set for \texttt{Cl} and \texttt{rem} parameters may ``disagree". In those cases, the preference is given to the \texttt{Cl} options once the user choose that specific set of classes to under-sample and not the other ones (even if the defined classes are not the larger ones). This means that, when making a decision on how many and which examples will be removed the first criteria used will be the \texttt{Cl} definition.


For a better clarification of the behavior stated we now provide some possible scenarios for multiclass problems and the corresponding expected behavior:

\begin{itemize}
\item \texttt{Cl} is set to one class which is neither the most nor the least frequent, and \texttt{rem} is set to ``maj". The expected behavior is the following:
- if a Tomek link exists connecting the largest class and another class(not included in \texttt{Cl}): no example is removed;
- if a Tomek link exists connecting the larger class and the class defined in \texttt{Cl}: the example from the \texttt{Cl} class is removed (because the user expressly indicates that only examples from class \texttt{Cl} should be removed);

\item \texttt{Cl} includes two classes and \texttt{rem} is set to ``both". This function will do the following:
- if a Tomek link exists between an example with class in \texttt{Cl} and another example with class not in \texttt{Cl}, then, only the example with class in \texttt{Cl} is removed;
- if the Tomek link exists between two examples with classes in \texttt{Cl}, then, both are removed.

\item \texttt{Cl} includes two classes and \texttt{rem} is set to ``maj". The behavior of this function is the following:
-if a Tomek link exists connecting two classes included in \texttt{Cl}, then only the example belonging to the more populated class is removed;
-if a Tomek link exists connecting an example from a class included in \texttt{Cl} and another example whose class is not in \texttt{Cl} and is the largest class, then, no example is removed.

\end{itemize}


We must also highlight that this strategy strongly depends on the distance metric considered for the nearest neighbors computation. We provide in \pUBL several different distance measures which are able to deal with numeric and/or nominal features, such as Manhattan distance, Euclidean Distance, HEOM or HVDM. For more details on the available distance functions check Section \ref{sec:distFunc}. The user may set the desired distance metric through the \texttt{dist} parameter.


The implementation provided in this package returns a list containing: the new data set modified through the Tomek links strategy and the indexes of the examples removed. Under certain situations, this strategy is not able to remove any example of the data set. In this case, a warning is issued to advert the user that no example was removed.

The following examples with ImbC data set show how Tomek links can be applied.

<<iris_TL>>=
# using the default parameters except for the distance function
# ImbC has nominal and numeric features and this requires the use of 
# specific distance functions
  ds <- TomekClassif(Class~., ImbC, dist="HEOM")
# using HEOM distance metric, and selecting only one class to under-sample
  dsHEOM <- TomekClassif(Class~., ImbC, dist="HEOM", 
                         Cl="normal")

# check the new dsHEOM data set
  summary(dsHEOM[[1]])

# check the indexes of the examples removed:
  dsHEOM[[2]]


# using HVDM distance, enable the removal of examples from all classes, and
# select to break the link by only removing the example from the majority class
  dsHVDMM <- TomekClassif(Class~., ImbC, dist="HVDM", Cl="all", rem="maj") 
# similar but breaking the Tomek link by removing both examples that for it
  dsHVDMB <- TomekClassif(Class~., ImbC, dist="HVDM", Cl="all", rem="both")

@

<<TL_table, echo=FALSE, results='asis'>>=

nm <- c("Original","ds", "dsHEOM", "dsHVDMM","dsHVDMB")

res <- c(table(ImbC$Class),table(ds[[1]]$Class), table(dsHEOM[[1]]$Class), table(dsHVDMM[[1]]$Class), table(dsHVDMB[[1]]$Class))

m <- matrix(res, nrow=5,ncol=3, byrow=TRUE, dimnames=list(nm, names(table(ImbC$Class))))

xt <-xtable(m, caption="Number of examples in each class for different Tomek Links parameters.",label="tab:TL_table")
print(xt,comment = FALSE, type = 'latex')

@


Figure \ref{fig:TL_difPar2} shows the impact in ImbC examples of the last experiences.
<<TL_difPar2, fig.cap="Impact of Tomek links strategy in ImbC synthetic data set. (top left: ImbC data; top right: ds data; bottom left:dsHVDMM data; and bottom right: dsHVDMB data)", echo=FALSE>>=

g1 <- ggplot(ImbC, aes(x = X2, y = X1)) + 
  geom_jitter(data = ImbC, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1)
       
g2 <- ggplot(ds[[1]], aes(x = X2, y = X1)) + 
  geom_jitter(data = ds[[1]], aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) 

g3 <- ggplot(dsHVDMM[[1]], aes(x = X2, y = X1)) + 
  geom_jitter(data = dsHVDMM[[1]], aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) 

g4 <- ggplot(dsHVDMB[[1]], aes(x = X2, y = X1)) + 
  geom_jitter(data = dsHVDMB[[1]], aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) 

plots <- list(g1,g2,g3,g4,ncol=2)

do.call(grid.arrange, plots)
@


Let us now consider the iris data set, changed with the goal of having an imbalanced distribution of the classes. 
<<ir_def>>=
data(iris)
dat <- iris[-c(61:85, 116:150),c(1,2,5)]
summary(dat)
@


Let us observe the impact of applying Tomek links strategy in this data.

<<ir_TL>>=
# using the default in all parameters
  ir <- TomekClassif(Species~., dat)
# using chebyshev distance metric, and selecting only two classes to under-sample
  irCheb <- TomekClassif(Species~., dat, dist="Chebyshev", 
                         Cl=c("virginica", "setosa"))
# using Manhattan distance, enable the removal of examples from all classes, and
# select to break the link by only removing the example from the majority class
  irManM <- TomekClassif(Species~., dat, dist="Manhattan", Cl="all", rem="maj") 
  irManB <- TomekClassif(Species~., dat, dist="Manhattan", Cl="all", rem="both")
@

<<irTL_table, echo=FALSE, results='asis'>>=

nm <- c("Original","ir", "irCheb", "irManM","irManB")

res <- c(table(dat$Species),table(ir[[1]]$Species), table(irCheb[[1]]$Species), table(irManM[[1]]$Species), table(irManB[[1]]$Species))

m <- matrix(res, nrow=5,ncol=3, byrow=TRUE, dimnames=list(nm, names(table(dat$Species))))

xt <-xtable(m, caption="Number of examples in each class for different Tomek Links parameters.",label="tab:irTL_table")
print(xt,comment = FALSE, type = 'latex')

@


Figure \ref{fig:TL_difPar} shows the impact of the previously described Tomek links strategies in the iris subset considered.

<<TL_difPar, fig.cap="Impact of Tomek links strategy in classes virginica and versicolor of the subset of iris data (top left: original iris subset; top right: irCheb data; bottom left: irManM data and bottom right: irManB data).", echo=FALSE, fig.height=7, fig.width=8>>=
dO <- subset(dat, Species!="setosa")
dC <- subset(irCheb[[1]], Species!="setosa")
dM <- subset(irManM[[1]], Species!="setosa")
dB <- subset(irManB[[1]], Species!="setosa")

g1 <- ggplot(dO,
       aes(x=dO[,1],
           y=dO[,2],
           colour=Species))+
  geom_point()+ xlab(colnames(dO)[1])+ylab(colnames(dO)[2]) + 
  ggtitle("original data")

g2 <- ggplot(dC,
       aes(x=dC[,1],
           y=dC[,2],
           colour=Species))+
  geom_point()+ xlab(colnames(dC)[1])+ylab(colnames(dC)[2]) + 
  ggtitle("Tomek links with Chebyshev distance")

g3 <- ggplot(dM,
       aes(x=dM[,1],
           y=dM[,2],
           colour=Species))+
  geom_point()+ xlab(colnames(dM)[1])+ylab(colnames(dM)[2]) + 
  ggtitle("Tomek links removing majority class")

g4 <- ggplot(dB,
       aes(x=dB[,1],
           y=dB[,2],
           colour=Species))+
  geom_point()+ xlab(colnames(dB)[1])+ylab(colnames(dB)[2])+
  ggtitle("Tomek links removing both examples")


plots <- list(g1,g2,g3,g4, ncol=2)

do.call(grid.arrange, plots)


@



\subsection{Condensed Nearest Neighbors}\label{sec:CNN}

The Condensed nearest neighbors rule (CNN) was presented by \cite{cnn}. The goal of this strategy is to perform under-sampling by building a subset of examples which is consistent with the original data. A subset is consistent with another if the elements in the subset classify correctly all the original examples using a 1-NN. 


To build a consistent subset we have adapted the proposal of \cite{KM97} to multiclass problems. The user starts by defining which are the most relevant classes in the data set using the \texttt{Cl} parameter. If the user prefers, an automatic option that corresponds to setting \texttt{Cl} to ``smaller", evaluates the distribution of the classes and determines which classes are candidates for being the smaller and most important. By default, this parameter is set to ``smaller" which means that the most relevant classes are automatically estimated from the data and correspond to those classes containing less than $\frac{\text{total number of examples}}{\text{number of classes}}$ examples. For instance, if a data set has 5 classes and a total number of examples of 100, the classes with less than 20 $(\frac{100}{5})$ examples will be considered the most important. The examples of the most relevant classes are then joined with one randomly selected example from each of the other classes. A 1-NN is computed with the distance metric provided by the user through the \texttt{dist} parameter. Then, all the examples from the original data set which were mislabeled in this procedure are also added to the reduced data set. This allows to obtain a smaller data set by removing examples from the larger and less important classes which are farther from the decision border.

This strategy is available through the \texttt{CNNClassif} function. This function returns a list containing: the modified data set, the classes that were considered important, and finally the unimportant classes.


We can now see some examples of this approach on the sythetic ImbC data and in the subset of iris data previously defined.

<<ImbC_CNN>>=
# select a distance that is appropriate for dealing with
# both nominal and numeric features
# the default considers the two minority classes as the most important ones
IHEOM <- CNNClassif(Class~., ImbC, dist="HEOM")

# considering only rare1 class is important
IHEOM1 <- CNNClassif(Class~., ImbC, dist="HEOM", Cl="rare1")

# considering only rare2 class as important
IHEOM2 <- CNNClassif(Class~., ImbC, dist="HEOM", Cl="rare2")

# use HVDM distance and the default of conisdering 
# both minority classes as the most important
IHVDM <- CNNClassif(Class~., ImbC, dist="HVDM")

# now we select rare1 as the important class
IHVDM1 <- CNNClassif(Class~., ImbC, dist="HVDM", Cl="rare1")

# this selects the class rare2 as the most important one
IHVDM2 <- CNNClassif(Class~., ImbC, dist="HVDM", Cl="rare2")
 @

<<ImbC_CNN_table, echo=FALSE, results='asis'>>=
nm <- c("Original", "IHEOM", "IHEOM1", "IHEOM2", "IHVDM", "IHVDM1", "IHVDM2")

res <- c(table(ImbC$Class),table(IHEOM[[1]]$Class), table(IHEOM1[[1]]$Class), table(IHEOM2[[1]]$Class), table(IHVDM[[1]]$Class), table(IHVDM1[[1]]$Class), table(IHVDM2[[1]]$Class))

m <- matrix(res, nrow=7,ncol=3, byrow=TRUE, dimnames=list(nm, names(table(ImbC$Class))))


xt <-xtable(m, caption="Number of examples in each class of IMbC pre-processed data sets for different CNN parameters.",label="tab:ImbC_CNN_table")
print(xt,comment = FALSE, type = 'latex')
@

Table~\ref{tab:ImbC_CNN_table} shows the number of examples that remain in each class of the pre-processed data sets. In this case, it is evident that both the distance function selected and the classes provided to be considered important have a significant impact on this strategy. 
Figures~\ref{fig:ImbC_CNN_HEOM} and \ref{fig:ImbC_CNN_HVDM} show the impact of the previously described strategies on ImbC data using HEOM and HVDM distances.

<<ImbC_CNN_HEOM, echo=FALSE, fig.cap="Impact on ImbC data of CNN method with HEOM distance for different values of parameter Cl.">>=
g1 <- ggplot(ImbC, aes(x = X2, y = X1)) + 
  geom_jitter(data = ImbC, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1)

g2 <- ggplot(IHEOM[[1]], aes(x = X2, y = X1)) + 
  geom_jitter(data = IHEOM[[1]], aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) 

g3 <- ggplot(IHEOM1[[1]], aes(x = X2, y = X1)) + 
  geom_jitter(data = IHEOM1[[1]], aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1)  

g4 <- ggplot(IHEOM2[[1]], aes(x = X2, y = X1)) + 
  geom_jitter(data = IHEOM2[[1]], aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1)  

plots <- list(g1,g2,g3,g4,ncol=2)

do.call(grid.arrange, plots)

@



<<ImbC_CNN_HVDM, echo=FALSE, fig.cap="Impact on ImbC data of CNN method with HVDM distance for different values of parameter Cl.">>=
g1 <- ggplot(ImbC, aes(x = X2, y = X1)) + 
  geom_jitter(data = ImbC, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("ImbC data")

g2 <- ggplot(IHVDM[[1]], aes(x = X2, y = X1)) + 
  geom_jitter(data = IHVDM[[1]], aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("IHVDM data")

g3 <- ggplot(IHVDM1[[1]], aes(x = X2, y = X1)) + 
  geom_jitter(data = IHVDM1[[1]], aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1)  +
  ggtitle("IHVDM1 data")

g4 <- ggplot(IHVDM2[[1]], aes(x = X2, y = X1)) + 
  geom_jitter(data = IHVDM2[[1]], aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1)  +
  ggtitle("IHVDM2 data")

plots <- list(g1,g2,g3,g4,ncol=2)

do.call(grid.arrange, plots)

@


Let us now consider the subset of iris data. In this case, the two features are numeric which allows the use of different distance functions.

<<Iris_CNN>>=

# just to remember the considered subset of iris data set
  summary(dat)

# use of the default distance: Euclidean 
  myCNN <- CNNClassif(Species~., dat, Cl=c("setosa", "virginica"))
  CNN1 <- CNNClassif(Species~., dat, Cl="smaller")
# try other distance functions
  CNN2 <- CNNClassif(Species~., dat, dist="Chebyshev", Cl="versicolor")
  CNN3 <- CNNClassif(Species~., dat, dist="HVDM", Cl="virginica")
  CNN4 <- CNNClassif(Species~., dat, dist="p-norm", p=3, Cl="setosa")

# check the new data set obtained in CNN1
summary(CNN1[[1]]$Species)

# check the classes which were considered important
CNN1[[2]]

# check the classes which were considered unimportant
CNN1[[3]]

@


<<CNN_table, echo=FALSE, results='asis'>>=

nm <- c("Original","myCNN", "CNN1", "CNN2","CNN3")

res <- c(table(dat$Species),table(myCNN[[1]]$Species), table(CNN1[[1]]$Species), table(CNN2[[1]]$Species), table(CNN3[[1]]$Species))

m <- matrix(res, nrow=5,ncol=3, byrow=TRUE, dimnames=list(nm, names(table(dat$Species))))


xt <-xtable(m, caption="Number of examples in each class for different CNN parameters.",label="tab:CNN_table")
print(xt,comment = FALSE, type = 'latex')


@



It is clear from these examples that this method entails a significant reduction on the number of examples left in the modified data set. Moreover, since there is a random selection of points belonging to the less important class(es) the obtained data set may differ for different runs. Figure \ref{fig:CNN_plot} provides a visual illustration of the impact of this method in the previously considered subset of iris data.

<<CNN_plot, echo=FALSE, fig.cap="Impact of CNN method for different values of parameter Cl and different distance functions on the subset of iris data (top left: original iris subset; top right:myCNN data; middle left: CNN1 data; middle right: CNN2 data; bottom left: CNN3 data; and bottom right: CNN4 data).">>=

g1 <- ggplot(dat,
       aes(x=dat[,1],
           y=dat[,2],
           colour=Species))+
  geom_point()+ xlab(colnames(dat)[1])+ylab(colnames(dat)[2]) + 
  xlim(min(dat[,1]-0.2), max(dat[,1]+0.2)) + ylim(min(dat[,2]-0.2), max(dat[,2]+0.2)) +
  ggtitle("original data")


g2 <- ggplot(myCNN[[1]],
       aes(x=myCNN[[1]][,1],
           y=myCNN[[1]][,2],
           colour=myCNN[[1]]$Species))+ guides(colour=guide_legend(title="Species") ) +
  geom_point()+ xlab(colnames(myCNN[[1]])[1])+ylab(colnames(myCNN[[1]])[2]) + 
  xlim(min(dat[,1]-0.2), max(dat[,1]+0.2)) + ylim(min(dat[,2]-0.2), max(dat[,2]+0.2)) +
  ggtitle("myCNN")


g3 <- ggplot(CNN1[[1]],
       aes(x=CNN1[[1]][,1],
           y=CNN1[[1]][,2],
           colour=CNN1[[1]]$Species))+ guides(colour=guide_legend(title="Species") ) +
  geom_point()+ xlab(colnames(CNN1[[1]])[1])+ylab(colnames(CNN1[[1]])[2]) + 
  xlim(min(dat[,1]-0.2), max(dat[,1]+0.2)) + ylim(min(dat[,2]-0.2), max(dat[,2]+0.2)) +
  ggtitle("CNN1 data")

g4 <- ggplot(CNN2[[1]],
       aes(x=CNN2[[1]][,1],
           y=CNN2[[1]][,2],
           colour=CNN2[[1]]$Species))+ guides(colour=guide_legend(title="Species") ) +
  geom_point()+ xlab(colnames(CNN2[[1]])[1])+ylab(colnames(CNN2[[1]])[2]) + 
  xlim(min(dat[,1]-0.2), max(dat[,1]+0.2)) + ylim(min(dat[,2]-0.2), max(dat[,2]+0.2)) +
  ggtitle("CNN2 data")

g5 <- ggplot(CNN3[[1]],
       aes(x=CNN3[[1]][,1],
           y=CNN3[[1]][,2],
           colour=CNN3[[1]]$Species))+ guides(colour=guide_legend(title="Species") ) +
  geom_point()+ xlab(colnames(CNN3[[1]])[1])+ylab(colnames(CNN3[[1]])[2]) + 
  xlim(min(dat[,1]-0.2), max(dat[,1]+0.2)) + ylim(min(dat[,2]-0.2), max(dat[,2]+0.2)) +
  ggtitle("CNN3 data")

g6 <- ggplot(CNN4[[1]],
       aes(x=CNN4[[1]][,1],
           y=CNN4[[1]][,2],
           colour=CNN4[[1]]$Species))+ guides(colour=guide_legend(title="Species") ) +
  geom_point()+ xlab(colnames(CNN4[[1]])[1])+ylab(colnames(CNN4[[1]])[2]) + 
  xlim(min(dat[,1]-0.2), max(dat[,1]+0.2)) + ylim(min(dat[,2]-0.2), max(dat[,2]+0.2)) +
  ggtitle("CNN4 data")


plots <- list(g1,g2,g3,g4,g5,g6,ncol=2)

do.call(grid.arrange, plots)


@


\subsection{One-sided Selection}\label{sec:OSS}


\cite{KM97} proposed a new method for modifying a given data set by applying the Tomek links under-sampling strategy and afterwards the CNN technique. \cite{batista2004study} also tested the reverse order for applying the techniques: first apply CNN method and then Tomek links. The main motivation for this was to apply Tomek links to an already reduced data set because Tomek links technique is a more computationally demanding task.

In \UBL we have gathered under the same function, \texttt{OSSClassif}, both techniques. To distinguish between the two methods, we included a parameter \texttt{start} which defaults to ``CNN". The user may therefore select the order in which we want to apply the two techniques: CNN and Tomek links. In this implementation, when Tomek links are applied, they always imply the removal of both examples forming the Tomek link. 

We have adapted both methods for dealing with multiclass imbalanced problems. To do so, we have included the parameter \texttt{Cl} which allows the user to specify the most important classes. Similarly to the behavior of CNN strategy, the user may define for the \texttt{Cl} parameter the value ``smaller". In this case, the most important classes are automatically determined using the same method presented in CNN strategy. When the relevant classes are chosen with this automatic method, the less frequent classes (which are considered the most relevant ones) are those which have a frequency below $\frac{number of examples}{number of classes}$. This means that all the classes with a frequency below the mean frequency of the data set classes is considered a minority class. The \texttt{OSSClassif} function also allows to specify which distance metric should be used in the neighbors computation. For more details on the available distance functions see Section \ref{sec:distFunc}. We must also mention that this strategy may potentially produce warnings due to the use of Tomek links strategy. As previously mentioned when Tomek links approach was presented, this method may not change the provided data set. In this case a warning is issued to advert the user. This warning may also occur when using OSS strategy if the Tomek links method produce it.


Let us observe how this method can be used with ImbC data.

<<ImbC_OSS>>=
# OSS method with HEOM distance
HEOM1 <- OSSClassif(Class~., ImbC, dist="HEOM")
HEOM2 <- OSSClassif(Class~., ImbC, dist="HEOM", start="Tomek", Cl="rare1")

# OSS method with HVDM distance
HVDM1 <- OSSClassif(Class~., ImbC, dist="HVDM", Cl="rare1")
HVDM2 <- OSSClassif(Class~., ImbC, dist="HVDM", start="Tomek", Cl="rare2")
@

Table~\ref{tab:ImbC_oss_table} shows the impact of OSS strategy with different parameters on the number of examples in each class of the pre-processed data sets and Figure~\ref{fig:ImbC_oss} shows the examples distribution on the pre-processed data sets.

<<ImbC_oss_table, echo=FALSE, results='asis'>>=
nm <- c("Original","HEOM1","HEOM2", "HVDM1","HVDM2")

res <- c(table(ImbC$Class),table(HEOM1$Class), table(HEOM2$Class), table(HVDM1$Class), table(HVDM2$Class))

m <- matrix(res, nrow=5, ncol=3, byrow=TRUE, dimnames=list(nm, names(table(ImbC$Class))))

xt <-xtable(m, caption="Number of examples in each class for different OSS parameters.",label="tab:ImbC_oss_table")
print(xt,comment = FALSE, type = 'latex')

@

<<ImbC_oss, echo=FALSE, fig.cap="Impact on ImbC data of OSS method with different parameters.">>=
g1 <- ggplot(ImbC, aes(x = X2, y = X1)) + 
  geom_jitter(data = ImbC, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) + 
  ggtitle("ImbC data")

g2 <- ggplot(HEOM1, aes(x = X2, y = X1)) + 
  geom_jitter(data = HEOM1, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("HEOM1 data")

g3 <- ggplot(HEOM2, aes(x = X2, y = X1)) + 
  geom_jitter(data = HEOM2, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("HEOM2 data") 

g4 <- ggplot(HVDM1, aes(x = X2, y = X1)) + 
  geom_jitter(data = HVDM1, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("HVDM1 data") 

g5 <- ggplot(HVDM2, aes(x = X2, y = X1)) + 
  geom_jitter(data = HVDM2, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("HVDM2 data")  

plots <- list(g1,g2,g3,g4,g5,ncol=2)

do.call(grid.arrange, plots)

@


The use of this method with data sets with all numeric features allows the use of several other distance functions. Let us briefly observe the impact of this method in the previously defined iris subset.

<<Iris_OSS>>=
set.seed(1234)

# using all the defaults
ir1 <- OSSClassif(Species~., dat)

# using distance functions only suitable for numeric features
ir2 <- OSSClassif(Species~., dat, dist="p-norm", p=3, 
                  Cl="virginica")
ir3 <- OSSClassif(Species~., dat, dist="Chebyshev", 
                  Cl=c("versicolor", "virginica"), start="Tomek")


summary(ir1$Species)
summary(ir2$Species)
summary(ir3$Species)

@


The results obtained with the variants of OSS method on iris subset can be visualized in Figure \ref{fig:OSS_plot}.


<<OSS_plot, fig.cap="OSS technique with different parameters applied to the imbalanced iris subset.", echo=FALSE, fig.height=7, fig.width=8>>=

g1 <- ggplot(dat,
       aes(x=dat[,1],
           y=dat[,2],
           colour=Species))+
  geom_point()+ xlab(colnames(dat)[1])+ylab(colnames(dat)[2]) + 
  xlim(min(dat[,1]-0.2), max(dat[,1]+0.2)) + ylim(min(dat[,2]-0.2), max(dat[,2]+0.2)) +
  ggtitle("original data")


g2 <- ggplot(ir1,
       aes(x=ir1[,1],
           y=ir1[,2],
           colour=ir1$Species))+ guides(colour=guide_legend(title="Species") ) +
  geom_point()+ xlab(colnames(ir1)[1])+ylab(colnames(ir1)[2]) + 
  xlim(min(dat[,1]-0.2), max(dat[,1]+0.2)) + ylim(min(dat[,2]-0.2), max(dat[,2]+0.2)) +
  ggtitle("ir1 data")


g3 <- ggplot(ir2,
       aes(x=ir2[,1],
           y=ir2[,2],
           colour=ir2$Species))+ guides(colour=guide_legend(title="Species") ) +
  geom_point()+ xlab(colnames(ir2)[1])+ylab(colnames(ir2)[2]) + 
  xlim(min(dat[,1]-0.2), max(dat[,1]+0.2)) + ylim(min(dat[,2]-0.2), max(dat[,2]+0.2)) +
  ggtitle("ir2 data")

g4 <- ggplot(ir3,
       aes(x=ir3[,1],
           y=ir3[,2],
           colour=ir3$Species))+ guides(colour=guide_legend(title="Species") ) +
  geom_point()+ xlab(colnames(ir3)[1])+ylab(colnames(ir3)[2]) + 
  xlim(min(dat[,1]-0.2), max(dat[,1]+0.2)) + ylim(min(dat[,2]-0.2), max(dat[,2]+0.2)) +
  ggtitle("ir3 data")



plots <- list(g1,g2,g3,g4,ncol=2)

do.call(grid.arrange, plots)

@





\subsection{Edited Nearest Neighbors}\label{sec:ENN}

The Edited Nearest Neighbor (ENN) algorithm was proposed by \cite{wilson1972asymptotic}. This method falls within the under-sampling approaches and has been used to address imbalanced classification problems. The original ENN algorithm uses a 3-NN classifier to remove the examples whose class is different from the class of at least two of its neighbors. 

We have implemented this approach for being able to tackle multiclass problems, allowing the user to specify through the \texttt{Cl} parameter a subset of classes which should be under-sampled. Moreover, in our implementation, the user may also define the number of nearest neighbors that should be considered by the algorithm. This means that an example is removed if its class label is different from the class label of at least half of its k-nearest neighbors and if it belongs to the subset of classes candidates for removal. The ENN algorithm is available in \UBL through the function \texttt{ENNClassif}. The number of neighbors to consider is set through the parameter \texttt{k} and the subset of classes that are candidates for being under-sampled are defined through the \texttt{Cl} parameter. The default of \texttt{Cl} is ``all", meaning that all classes are candidates for having examples removed. The user can also specify which distance metric he wants to use in the nearest neighbors computation. The function \texttt{ENNClassif} returns a list containing the new under-sampled data set and the indexes of the examples removed.

It is possible that ENN finds no examples to remove, which means that, for the parameters selected, there are no examples satisfying the necessary conditions to be removed. In this case, a warning is issued with the goal of adverting the user that the strategy is not modifying the data set provided.


We can use this strategy in ImbC data as follows:


<<ImbC_ENN>>=
# use of default parameters except for the distance function
ENN1 <- ENNClassif(Class~., ImbC, dist="HVDM")

ENN2 <- ENNClassif(Class~., ImbC, dist="HVDM", Cl="rare1")

ENN3 <- ENNClassif(Class~., ImbC, dist="HVDM", Cl="rare2")

# now using the HEOM distance
ENN4 <- ENNClassif(Class~., ImbC, dist="HEOM")

# vary the number of neighbors considered by this method
ENN5 <- ENNClassif(Class~., ImbC, k=5, dist="HEOM")

ENN6 <- ENNClassif(Class~., ImbC, k=1, dist="HEOM")

@


Table~\ref{tab:ImbC_ENN_table} shows the impact of ENN strategy on ImbC synthetic data set with different parameters and Figure~\ref{fig:ImbC_ENN_plot} illustrates the examples distribution in the original and changed data sets.

<<ImbC_ENN_table, results='asis', echo=FALSE>>=
nm <- c("Original","ENN1", "ENN2","ENN3", "ENN4", "ENN5", "ENN6")

res <- c(table(ImbC$Class),table(ENN1[[1]]$Class), table(ENN2[[1]]$Class), table(ENN3[[1]]$Class), table(ENN4[[1]]$Class), table(ENN5[[1]]$Class), table(ENN6[[1]]$Class))

m <- matrix(res, nrow=7,ncol=3, byrow=TRUE, dimnames=list(nm, names(table(ImbC$Class))))

xt <-xtable(m, caption="Number of examples in each class for different parameters of ENN strategyapplied in ImbC data.",label="tab:ImbC_ENN_table")
print(xt,comment = FALSE, type = 'latex')
@

<<ImbC_ENN_plot, echo=FALSE, fig.cap="Impact on ImbC data of ENN method with different parameters.">>=
# g1 <- ggplot(ImbC, aes(x = X2, y = X1)) + 
#   geom_jitter(data = ImbC, aes(colour=Class, pch=Class)) + 
#   scale_color_manual(values=c("lightgreen", "blue", "orange")) +
#   scale_shape_manual(values=c(0,16,17)) + 
#   ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) + 
#   ggtitle("ImbC data")

g2 <- ggplot(ENN1[[1]], aes(x = X2, y = X1)) + 
  geom_jitter(data = ENN1[[1]], aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("ENN1 data")

g3 <- ggplot(ENN2[[1]], aes(x = X2, y = X1)) + 
  geom_jitter(data = ENN2[[1]], aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("ENN2 data") 

g4 <- ggplot(ENN3[[1]], aes(x = X2, y = X1)) + 
  geom_jitter(data = ENN3[[1]], aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("ENN3 data") 

g5 <- ggplot(ENN4[[1]], aes(x = X2, y = X1)) + 
  geom_jitter(data = ENN4[[1]], aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("ENN4 data")  

g6 <- ggplot(ENN5[[1]], aes(x = X2, y = X1)) + 
  geom_jitter(data = ENN5[[1]], aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("ENN5 data")  

g7 <- ggplot(ENN6[[1]], aes(x = X2, y = X1)) + 
  geom_jitter(data = ENN6[[1]], aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("ENN6 data")  

plots <- list(g2,g3,g4,g5,g6,g7,ncol=2)

do.call(grid.arrange, plots)


@

We can also use this strategy on the imbalanced iris subset previously defined. In this case, given that the data contains only numeric features it is possible to use different distance functions.


<<Iris_ENN>>=
  set.seed(123)
  Man5 <- ENNClassif(Species~., dat, k=5, dist="Manhattan", Cl="all") 
  Default <- ENNClassif(Species~., dat)
  ChebSub7 <- ENNClassif(Species~., dat, k=7, dist="Chebyshev", 
                         Cl=c("virginica", "setosa"))
  ChebAll7 <- ENNClassif(Species~., dat, k=7, dist="Chebyshev")
  HVDM3 <- ENNClassif(Species~., dat, k=3, dist="HVDM")
@

In Table \ref{tab:iris_ENN_table} we can observe the examples distributions for some parameters settings in ENN strategy and in Figure \ref{fig:ir_ENN_plot} we can visualize that distribution.

<<iris_ENN_table, results='asis', echo=FALSE>>=
nm <- c("Original","Man5", "Default","ChebSub7", "ChebAll7", "HVDM3")

res <- c(table(dat$Species),table(Man5[[1]]$Species), table(Default[[1]]$Species), table(ChebSub7[[1]]$Species), table(ChebAll7[[1]]$Species), table(HVDM3[[1]]$Species))

m <- matrix(res, nrow=6,ncol=3, byrow=TRUE, dimnames=list(nm, names(table(Man5[[1]]$Species))))

xt <-xtable(m, caption="Number of examples in each class for different parameters of ENN strategy.",label="tab:iris_ENN_table")
print(xt,comment = FALSE, type = 'latex')
@


<<ir_ENN_plot, echo=FALSE, fig.cap="Impact in the subset of iris data of several parameters for ENN strategy.",out.height="0.5\\textheight">>=
DF <- rbind(data.frame(Dat="Original",obs=as.factor(dat$Species)), data.frame(Dat="Manhattan5NN", obs=as.factor(Man5[[1]]$Species)), data.frame(Dat="Default", obs=as.factor(Default[[1]]$Species)), data.frame(Dat="Chebyshev7NNSub", obs=as.factor(ChebSub7[[1]]$Species)), data.frame(Dat="Chebyshev7NNAll", obs=as.factor(ChebAll7[[1]]$Species)), data.frame(Dat="HVDM3", obs=as.factor(HVDM3[[1]]$Species)))

g1 <- ggplot(DF,aes(x=obs, fill=Dat, colour=Dat))+geom_bar(position="dodge", aes(group=Dat), colour="black")


g2 <- ggplot(DF,aes(x=Dat, fill=obs, colour=obs))+geom_bar(position="fill", aes(group=obs), colour="black")+theme(axis.text.x  = element_text(angle=90, vjust=0.5))

plots <- list(g1, g2)

do.call(grid.arrange, plots)

@


This strategy has an unexpected behavior at first sight. In fact, the ENN method has further reduced the already minority classes. This can be explained by the goal of the ENN method which, being a cleaning technique, discards examples which may introduce errors no mater to which class they belong. As we know, in the iris data set the classes versicolor and virginica are the ones which are more difficult to classify. Therefore, the applied ENN strategy will try to remove examples exactly from those two classes.


Sometimes, ENN method is not capable of removing any example. When this happens, the original data set remains unchanged and an warning is issued. This warning is made only to advert the user that no examples were removed. On the other hand, with some data sets, this algorithm may completely remove one or more classes. This behavior may jeopardize the use of standard learning algorithms because they are provided with data set with only one class in the target variable. To overcome this issue, when a class is completely removed with the ENN strategy we randomly chose one example of that class to add to the under-sampled data set.


\subsection{Neighborhood Cleaning Rule}\label{sec:NCL}

The Neighborhood Cleaning Rule (NCL) algorithm was proposed in \cite{laurikkala2001improving}. This approach starts by splitting the data set $D$ in two: a subset $C$ with the examples belonging to the most important (an usually less frequent) class(es) and another subset $O$ containing the examples from the less important class(es). A new set $A_1$ of examples is formed with the noisy examples belonging to the subset $O$ which are identified using the ENN method.
Then, another set $A_2$ of examples is built as follows. For each class $C_i$ in $O$, the k nearest neighbors of each example in $C_i$ are scanned. The example is included in $A_2$ if all the scanned k nearest neighbors have a class label not contained in $C$ and if the example belongs to a class which has a cardinal of at least $\frac{1}{2}$ of the cardinal of smaller class in $C$. This last constraint forces the algorithm to keep the examples of classes with to few examples.
Finally, the examples in $A_1$ and $A_2$ are removed from the original data set.

Since this strategy internally uses the ENN approach we highlight that it is possible that warnings are issued. As mentioned before, the user is always adverted if ENN does not alter the data set. This can also happen with NCL if internally the ENN does not remove any example.

The NCL approach is available in \UBL through the \texttt{NCLClassif} function. In addition to providing a formula describing the prediction problem (\texttt{form}) and a data set (\texttt{dat}) the user may set the parameters corresponding to the number of neighbors considered (\texttt{k}), the distance function used (\texttt{dist}) and the classes that should be under-sampled (\texttt{Cl}). This last parameter may be set to \texttt{smaller}. In this case, the smaller classes are automatically estimated, and assumed to be the most important ones. All the other least important classes are candidates for the under-sampling of NCL method to be applied. We now provide some examples of application of the NCL method.


We will begin using the ImbC data set. As this data contains numeric and nominal features it is necessary to use suitable distance functions, such as "HEOM" or "HVDM".

<<ImbC_NCL>>=
IHEOM1 <- NCLClassif(Class~., ImbC, k=10, dist="HEOM", Cl="smaller")
IHEOM2 <- NCLClassif(Class~., ImbC, k=1, dist="HEOM")
IHEOM3 <- NCLClassif(Class~., ImbC, k=1, dist="HEOM", Cl="rare1")

IHVDM1<- NCLClassif(Class~., ImbC, k=10, dist="HVDM", Cl="smaller")
IHVDM2<- NCLClassif(Class~., ImbC, k=5, dist="HVDM", Cl="rare1")
IHVDM3<- NCLClassif(Class~., ImbC, k=1, dist="HVDM", Cl="rare2")

@

Table~\ref{tab:ImbC_NCL_table} summarizes the impact produced in the number of examples in the classes on the new data sets chenaged through NCL strategy and Figure~\ref{fig:ImbC_NCL_plot} show the examples distributions in these data sets.


<<ImbC_NCL_table, results='asis', echo=FALSE>>=
nm <- c("Original", "IHEOM1","IHEOM2","IHEOM3","IHVDM1","IHVDM2","IHVDM3")

res <- c(table(ImbC$Class),table(IHEOM1$Class),table(IHEOM2$Class),table(IHEOM3$Class),table(IHVDM1$Class),table(IHVDM2$Class),table(IHVDM3$Class))

m <- matrix(res, nrow=7, ncol=3, byrow=TRUE, dimnames=list(nm, names(table(ImbC$Class))))

xt <-xtable(m, caption="Number of examples in each class for different parameters of NCL strategyon ImbC data set.",label="tab:ImbC_NCL_table")
print(xt,comment = FALSE, type = 'latex')

@


<<ImbC_NCL_plot, echo=FALSE, fig.cap="Impact on ImbC data of NCL method with different parameters.">>=
# g1 <- ggplot(ImbC, aes(x = X2, y = X1)) + 
#   geom_jitter(data = ImbC, aes(colour=Class, pch=Class)) + 
#   scale_color_manual(values=c("lightgreen", "blue", "orange")) +
#   scale_shape_manual(values=c(0,16,17)) + 
#   ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) + 
#   ggtitle("ImbC data")

g2 <- ggplot(IHEOM1, aes(x = X2, y = X1)) + 
  geom_jitter(data = IHEOM1, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("IHEOM1 data")

g3 <- ggplot(IHEOM2, aes(x = X2, y = X1)) + 
  geom_jitter(data = IHEOM2, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("IHEOM2 data")

g4 <- ggplot(IHEOM3, aes(x = X2, y = X1)) + 
  geom_jitter(data = IHEOM3, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("IHEOM3 data")

g5 <- ggplot(IHVDM1, aes(x = X2, y = X1)) + 
  geom_jitter(data = IHVDM1, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("IHVDM1 data")

g6 <- ggplot(IHVDM2, aes(x = X2, y = X1)) + 
  geom_jitter(data = IHVDM2, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("IHVDM2 data")

g7 <- ggplot(IHVDM3, aes(x = X2, y = X1)) + 
  geom_jitter(data = IHVDM3, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("IHVDM3 data")

plots <- list(g2,g3,g4,g5,g6,g7,ncol=2)

do.call(grid.arrange, plots)

@

Let us now observe how this technique can be used in the imbalanced iris subset previously defined.


<<Iris_NCL>>=
set.seed(1234)
ir.M1 <- NCLClassif(Species~., dat, k=3, dist="p-norm", p=1, Cl="smaller")
ir.M2<- NCLClassif(Species~., dat, k=1, dist="p-norm", p=1, Cl="setosa")
ir.Def <- NCLClassif(Species~., dat)
ir.Ch <- NCLClassif(Species~., dat, k=7, dist="Chebyshev", Cl="virginica")
ir.Eu <- NCLClassif(Species~., dat, k=3, dist="Euclidean", 
                    Cl=c("setosa", "virginica"))
@


Table \ref{tab:iris_NCL_table} provides the number of examples in each class for different parameters of NCL method and in Figure \ref{fig:NCL_plot} the changes produced by the use of this method may be visualized.


<<iris_NCL_table, results='asis', echo=FALSE>>=
nm <- c("Original","ir.M1", "ir.M2","ir.Def", "ir.Ch", "ir.Eu")

res <- c(table(dat$Species),table(ir.M1$Species), table(ir.M2$Species), table(ir.Def$Species), table(ir.Ch$Species), table(ir.Eu$Species))

m <- matrix(res, nrow=6, ncol=3, byrow=TRUE, dimnames=list(nm, names(table(Man5[[1]]$Species))))

xt <-xtable(m, caption="Number of examples in each class for different parameters of NCL strategy.",label="tab:iris_NCL_table")
print(xt,comment = FALSE, type = 'latex')
@


<<NCL_plot,fig.cap="NCL techniques applied to a multiclass imbalanced problem.", echo=FALSE>>=
g1 <- ggplot(dat, aes(x=dat[,1], y=dat[,2],color=dat[,3]))+xlab("Length")+ylab("Width")+geom_point()+ggtitle("Original data")+scale_color_discrete(name="Species")
g2 <- ggplot(ir.M1, aes(x=ir.M1[,1], y=ir.M1[,2],color=ir.M1[,3]))+xlab("Length")+ylab("Width")+geom_point()+ggtitle("Manhattan distance and 3-NN")+scale_color_discrete(name="Species")
g3 <- ggplot(ir.M2, aes(x=ir.M2[,1], y=ir.M2[,2],color=ir.M2[,3]))+xlab("Length")+ylab("Width")+geom_point()+ggtitle("Manhattan distance and 1-NN")+scale_color_discrete(name="Species")
g4 <- ggplot(ir.Def, aes(x=ir.Def[,1], y=ir.Def[,2],color=ir.Def[,3]))+xlab("Length")+ylab("Width")+geom_point()+ggtitle("Default values")+scale_color_discrete(name="Species")
g5 <- ggplot(ir.Ch, aes(x=ir.Ch[,1], y=ir.Ch[,2],color=ir.Ch[,3]))+xlab("Length")+ylab("Width")+geom_point()+ggtitle("Chebyshev dist., 7-NN")+scale_color_discrete(name="Species")
g6 <- ggplot(ir.Eu, aes(x=ir.Eu[,1], y=ir.Eu[,2],color=ir.Eu[,3]))+xlab("Length")+ylab("Width")+geom_point()+ggtitle("Euclidean distance, 3-NN")+scale_color_discrete(name="Species")

plots <- list(g1, g2, g3, g4, g5, g6)

do.call(grid.arrange, plots)
@

\subsection{Generation of synthetic examples by the introduction of Gaussian Noise}\label{sec:gnClassif}

The use of Gaussian Noise to introduce a small perturbation in the data set examples was proposed by \cite{lee1999regularization} and then extended in \cite{lee2000noisy}. The proposed method consisted of producing replicas of the examples of the minority class by introducing normally distributed noise. In this approach, the majority class remained unchanged while the minority class was increased. The noise introduced depends on a fraction of the standard deviation of each numeric feature.



We have adapted this technique to multiclass imbalanced problems. Moreover, we have also included the possibility of combining this over-sampling procedure with the random under-sampling technique described in Section \ref{sec:RUClassif}. 

Regarding the over-sampling method, a new example from an important class is obtained by perturbing each numeric feature according to a random value following a normally distributed percentage of its standard deviation (with the standard deviation evaluated on the examples of that class). This means that, for a given value of \texttt{pert} defined by the user, each feature value ($i$) of the new example ($new_i$) is built as follows: $new_i=ex_i+rnorm(0,sd(i)\times pert) $, where $ex_i$ represents the original example value for feature $i$, and $sd(i)$ represents the evaluated standard deviation for feature $i$ in the class under consideration. For nominal features, the new example selects a label with a probability directly proportional to the frequency of the existing labels(with the frequency evaluated on the examples of that class).

The user may express which are the most relevant and the less important classes of the data set through the parameter \texttt{C.perc}. With this parameter the user also indicates the percentages of under and over-sampling to apply in each class. If a class is not referred in this parameter it will remain unchanged. Moreover, this parameter can also be set to ``balance" or ``extreme", cases where the under and over-sampling percentages are automatically estimated to achieve a balanced data set or a data set with the frequencies of the classes inverted. The perturbation applied to the numeric features is set using the \texttt{pert} parameter. Finally, the user may also specify if, when performing the random under-sampling strategy, it is allowed to perform sampling with repetition or not.

We now present an example of the impact of applying this technique for different values of the parameters in IMbC data.

<<ImbC_GN>>=
# using the default parameters that balance the problem classes
GN1 <-GaussNoiseClassif(Class~., ImbC)

# increase the neighborhood radius for generating the new synthetic examples
# the default is pert = 0.1
GN2 <- GaussNoiseClassif(Class~., ImbC, pert = 0.5)
# select the percentages to apply in each class
GN3 <- GaussNoiseClassif(Class~., ImbC, 
                         C.perc = list("normal" = 0.5, "rare1" = 10, "rare2" = 3))

#select the re-sampling percentages and the perturbation radius 
GN4 <- GaussNoiseClassif(Class~., ImbC, 
                         C.perc = list("normal" = 0.3, "rare1" = 5, "rare2" = 2),
                         pert = 0.05)

# use the option the inverts the classes frequencies
GN5 <- GaussNoiseClassif(Class~., ImbC, C.perc="extreme")
@



Table \ref{tab:iris_GN_table} presents the impact on the number of examples for the considered parameters of this strategy. In Figure \ref{fig:ir_GN_plot} we can observe the number of examples on the changed data sets for the parameters considered and Figure \ref{fig:ir_GN_plot2} presents the distribution of examples for those parameters.



<<iris_GN_table, results='asis',echo=FALSE>>=
nm <- c("Original", "GN1", "GN2","GN3", "GN4", "GN5")

nm2 <- list(ImbC, GN1, GN2, GN3, GN4, GN5)

res <- c(table(ImbC$Class), table(GN1$Class), table(GN2$Class), table(GN3$Class), table(GN4$Class), table(GN5$Class))

m <- matrix(res, nrow=6,ncol=3, byrow=TRUE, dimnames=list(nm, names(table(ImbC$Class))))

xt <-xtable(m, caption="Number of examples in each class for different parameters of Gaussian Noise strategy applied in ImbC data.",label="tab:iris_GN_table")
print(xt,comment = FALSE, type = 'latex')

@



<<ir_GN_plot, echo=FALSE, fig.cap="Impact in the Original ImbC data set of several parameters in Gaussian noise strategy. ",out.height="0.5\\textheight">>=
DF <- rbind(data.frame(Dat="Original",obs=ImbC$Class), data.frame(Dat="GN1", obs=GN1$Class), data.frame(Dat="GN2", obs=GN2$Class), data.frame(Dat="GN3", obs=GN3$Class), data.frame(Dat="GN4", obs=GN4$Class), data.frame(Dat="GN5", obs=GN5$Class))
g1 <- ggplot(DF,aes(x=obs, fill=Dat, colour=Dat))+geom_bar( position="dodge", aes(group=Dat), colour="black")

g2 <- ggplot(DF,aes(x=Dat, fill=obs, colour=obs))+geom_bar( position="fill", aes(group=obs), colour="black")

plots <- list(g1, g2)

do.call(grid.arrange, plots)

@



<<ir_GN_plot2, echo=FALSE, fig.cap="Impact on the examples distribution of ImbC data for different parameters in Gaussian Noise strategy.", warning=FALSE>>=


g1 <- ggplot(ImbC, aes(x = X2, y = X1)) + 
  geom_jitter(data = ImbC, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) + 
  ggtitle("ImbC data")

g2 <- ggplot(GN1, aes(x = X2, y = X1)) + 
  geom_jitter(data = GN1, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("GN1 data")

g3 <- ggplot(GN2, aes(x = X2, y = X1)) + 
  geom_jitter(data = GN2, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("GN2 data")

g4 <- ggplot(GN3, aes(x = X2, y = X1)) + 
  geom_jitter(data = GN3, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("GN3 data")

g5 <- ggplot(GN4, aes(x = X2, y = X1)) + 
  geom_jitter(data = GN4, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("GN4 data")


g6 <- ggplot(GN5, aes(x = X2, y = X1)) + 
  geom_jitter(data = GN5, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("GN5 data")

plots <- list(g1,g2,g3,g4,g5,g6,ncol=2)

do.call(grid.arrange, plots)

@

\subsection{The Smote Algorithm}\label{sec:smoteClassif}

The well known Smote algorithm was proposed by \cite{CBOK02}. This algorithm presents a new strategy to address the problem of imbalanced domains through the generation of synthetic examples. The new synthetic cases are generated by interpolation of two cases from the minority (positive) class. To obtain a new example from the minority class, the algorithm uses a seed example from that class, and randomly selects one of its k nearest neighbors. Then, having the two examples, a new synthetic case is obtained by interpolating the examples features. This procedure is illustrated in Figure \ref{fig:smote_illust}.

<<smote_illust, echo=FALSE, fig.cap="Generation of synthetic examples through Smote algorithm.\\label{smote_illust}", out.height="0.3\\textheight", out.width="0.6\\textwidth">>=
#generate an artificial data set to illustrate the smote algorithm
set.seed(234)
plot(1:40, rnorm(40,2),pch="-", col="blue",xlab="", ylab="", xaxt="n", yaxt="n")
points(1:40, rnorm(40,2), pch="-", col="blue")
points(1:40, rnorm(40,2), pch="-", col="blue")
set.seed(123)
points(c(10:32, 17, 17,19), c(rnorm(24,-0.3,0.5),0,0.5), pch="+", col="red")
lines(c(17,19), c(0,0.5), col="orange", lwd=2)
draw.circle(19,0.5,4.7,nv=100,border=NULL,col=NA,lty=3,lwd=1)
points(17.6, 0.25*17.6-4.25, pch="+")
@

This over-sampling strategy was also combined with random under-sampling of the majority class in \cite{CBOK02}. 


Our implementation of this method is available through the \texttt{SmoteClassif} function and is able to deal with multiclass tasks. The user can specify which are the most important and the less relevant classes using the \texttt{C.perc} parameter. Using the same parameter the user also expresses the percentages of over and under-sampling that should be applied to each class. When the data set includes nominal features, the interpolation of two examples for these features is solved by randomly selecting among the two values of the seed examples. Two automatic methods are provided for both the estimation of the relevant classes and the percentages of over and under-sampling to apply. These methods are available through the \texttt{C.perc} parameter which can be set to ``balance" or ``extreme". In both cases, it is ensured that the new obtained data set includes roughly the same number of examples as the original data set. When ``balance" or ``extreme" are chosen, both the minority/majority classes and the percentages of over/under-sampling are automatically estimated. The ``balance" option provides a balanced data set and the ``extreme" option provides a data set with the classes frequencies inverted, i.e., the most frequent classes in the original data set are the less frequent on the new data set and vice-versa.

Finally, the user may also express if the under-sampling process may include repetition of examples or not (using the \texttt{repl} parameter), may choose the number of nearest neighbors to use (parameter \texttt{k}) and can select the distance metric to be used in the nearest neighbors evaluation (parameter \texttt{dist}). 

The following example shows how this strategy can be used to modify the synthetic ImbC data set.


<<ImbC_smote>>=

IC1 <- SmoteClassif(Class~., ImbC, dist = "HEOM")

IC2 <- SmoteClassif(Class~., ImbC, k = 1, dist="HEOM")

IC3 <- SmoteClassif(Class~., ImbC, 
                    C.perc = list("normal" = 0.4, "rare1" = 8, "rare2" = 6),
                    dist = "HEOM")

IC4 <- SmoteClassif(Class~., ImbC, dist = "HVDM")

IC5 <- SmoteClassif(Class~., ImbC, k = 1, dist = "HVDM")

# class rare2 is not refered in the C.perc parameter. This means that
# this class will remain unchanged
IC6 <- SmoteClassif(Class~., ImbC, dist = "HVDM",
                    C.perc = list("normal"=0.2, "rare1"=10))

IC7 <- SmoteClassif(Class~., ImbC, dist = "HVDM", C.perc="extreme")
@



Table \ref{tab:iris_smote_table} show the impact on the number of examples in each class for several parameters of smote technique.

<<smote_table1, echo=FALSE, results='asis'>>=
nm <- c("Original", "IC1", "IC2","IC3", "IC4", "IC5", "IC6", "IC7")

res <- c(table(ImbC$Class), table(IC1$Class), table(IC2$Class), table(IC3$Class), table(IC4$Class), table(IC5$Class), table(IC6$Class), table(IC7$Class))

m <- matrix(res, nrow=8,ncol=3, byrow=TRUE, dimnames=list(nm, names(table(ImbC$Class))))

xt <-xtable(m, caption="Number of examples in each class for different parameters of smote strategy.",label="tab:iris_smote_table")
print(xt,comment = FALSE, type = 'latex')

@

Figures \ref{fig:smote_plot_hist} and \ref{fig:smote_plot} present the impact of applying smote strategy on an imbalanced data set.

<<smote_plot_hist, echo=FALSE, fig.cap="Impact in the Original data set of several parameters in smote strategy. ",out.height="0.5\\textheight">>=
DF <- rbind(data.frame(Dat="Original",obs=ImbC$Class), data.frame(Dat="IC1", obs=IC1$Class), data.frame(Dat="IC2", obs=IC2$Class), data.frame(Dat="IC3", obs=IC3$Class), data.frame(Dat="IC4", obs=IC4$Class), data.frame(Dat="IC5", obs=IC5$Class), data.frame(Dat="IC6", obs=IC6$Class), data.frame(Dat="IC7", obs=IC7$Class))
g1 <- ggplot(DF,aes(x=obs, fill=Dat, colour=Dat))+geom_bar( position="dodge", aes(group=Dat), colour="black")


g2 <- ggplot(DF,aes(x=Dat, fill=obs, colour=obs))+geom_bar( position="fill", aes(group=obs), colour="black")

plots <- list(g1, g2)

do.call(grid.arrange, plots)
@


<<smote_plot, fig.cap="Smote strategy applied to ImbC data with different parameters.", echo=FALSE, fig.width=7, fig.height=7>>=

g1 <- ggplot(ImbC, aes(x = X2, y = X1)) + 
  geom_jitter(data = ImbC, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) + 
  ggtitle("ImbC data")

g2 <- ggplot(IC1, aes(x = X2, y = X1)) + 
  geom_jitter(data = IC1, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("IC1 data")

g3 <- ggplot(IC2, aes(x = X2, y = X1)) + 
  geom_jitter(data = IC2, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("IC2 data")

g4 <- ggplot(IC3, aes(x = X2, y = X1)) + 
  geom_jitter(data = IC3, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("IC3 data")

g5 <- ggplot(IC4, aes(x = X2, y = X1)) + 
  geom_jitter(data = IC4, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("IC4 data")


g6 <- ggplot(IC5, aes(x = X2, y = X1)) + 
  geom_jitter(data = IC5, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("IC5 data")

g7 <- ggplot(IC6, aes(x = X2, y = X1)) + 
  geom_jitter(data = IC6, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("IC6 data")

g8 <- ggplot(IC7, aes(x = X2, y = X1)) + 
  geom_jitter(data = IC7, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("IC7 data")

plots <- list(g1,g2,g3,g4,g5,g6,g7,g8,ncol=2)

do.call(grid.arrange, plots)

@


\subsection{ADASYN Algorithm}\label{sec:adasynClassif}

The ADASYN algorithm was proposed by~\cite{he2008adasyn} for addressing the problem of imbalanced domains. ADASYNs' key idea is to use, for different minority class cases, a different weight that is associated with their level of difficulty in learning. More synthetic minority class examples are generated for the minority examples that are harder to learn while less sytnhetic examples are generated for the easier to learn. To determine the number of new examples to be generated for each minority class case the authors build a density distribution that is obtained as follows:

i) for each case $x_i$ determine the number $d_i \leq k$ of k-nearest neighbours that belong to the majority class;

ii) calculate $r_i = d_i/k$

iii) When all $r_i$ are calculated, obtain $\hat{r}_i$ by normalizing $r_i$ so that $\hat{r}_i$ is a density distribution.


Now, each case has an associated weight ($\hat{r}_i$) that is used as follows:

i) calculate the total number $G$ of new synthetic examples that are necessary to generate, according to the problem imbalance ratio and the user provided parameter beta that sets the balance desired in new distribution.

ii) for example $x_i$ generate $\hat{r}_i \times G$ new examples.

The new synthetic examples are obtained through SMOTE algorithm~\cite{CBOK02}.


The authors of ADASYN algorithm claim that this algorithm is able: i) to reduce the learning bias introduced by the original imbalanced distribution; and ii) to  adaptively shift the decision border towards the most difficult examples.


ADASYN algorithm was proposed for binary classification problems~\cite{he2008adasyn}. In the UBL package we have extended the presented ADASYN strategy and have also implemented it for multiclass problems. To achieve this we introduced parameter \texttt{baseClass} that allows the user to specify which is the reference class, i.e., the class against which all other classes will be compared to. If the user does not specifies this parameter, then the reference class is assumed to be most frequent class. The ADASYN algorithm also requires that a parameter \texttt{beta} is specified by the user. This parameter sets the desired balance level to obtain after the synthetic examples generation. In a binary class setting a single number is sufficient for beta. When beta is set to 1, the default, then the new data set to obtain will have the classes roughly balanced. If a single number is also used in a ulticlass setting, it will be used to express the balance level of each class against the \texttt{baseClass}. We also allow the user to provide a named list containing the classes names and the corresponding beta value to use.  


Let us now observe the impact of aplying the ADADYN algorithm to the synthetic ImbC data set.


<<ImbC_adasyn>>=

Adas1 <- AdasynClassif(Class~., ImbC, dist = "HEOM")

Adas2 <- AdasynClassif(Class~., ImbC, k = 1, dist="HEOM")

Adas3 <- AdasynClassif(Class~., ImbC, 
                    C.perc = list("normal" = 0.4, "rare1" = 8, "rare2" = 6),
                    dist = "HEOM")

Adas4 <- AdasynClassif(Class~., ImbC, dist = "HVDM")

Adas5 <- AdasynClassif(Class~., ImbC, k = 1, dist = "HVDM")

# class rare2 is not refered in the C.perc parameter. This means that
# this class will remain unchanged
Adas6 <- AdasynClassif(Class~., ImbC, dist = "HVDM",
                    C.perc = list("normal"=0.2, "rare1"=10))

Ada7 <- AdasynClassif(Class~., ImbC, dist = "HVDM", C.perc="extreme")
@



Table \ref{tab:iris_adasyn_table} show the impact on the number of examples in each class for several parameters of Adasyn technique.

<<smote_table, echo=FALSE, results='asis'>>=
nm <- c("Original", "IC1", "IC2","IC3", "IC4", "IC5", "IC6", "IC7")

res <- c(table(ImbC$Class), table(IC1$Class), table(IC2$Class), table(IC3$Class), table(IC4$Class), table(IC5$Class), table(IC6$Class), table(IC7$Class))

m <- matrix(res, nrow=8,ncol=3, byrow=TRUE, dimnames=list(nm, names(table(ImbC$Class))))

xt <-xtable(m, caption="Number of examples in each class for different parameters of smote strategy.",label="tab:iris_smote_table")
print(xt,comment = FALSE, type = 'latex')

@

Figures \ref{fig:smote_plot_hist2} and \ref{fig:smote_plot2} present the impact of applying smote strategy on an imbalanced data set.

<<smote_plot_hist2, echo=FALSE, fig.cap="Impact in the Original data set of several parameters in smote strategy. ",out.height="0.5\\textheight">>=
DF <- rbind(data.frame(Dat="Original",obs=ImbC$Class), data.frame(Dat="IC1", obs=IC1$Class), data.frame(Dat="IC2", obs=IC2$Class), data.frame(Dat="IC3", obs=IC3$Class), data.frame(Dat="IC4", obs=IC4$Class), data.frame(Dat="IC5", obs=IC5$Class), data.frame(Dat="IC6", obs=IC6$Class), data.frame(Dat="IC7", obs=IC7$Class))
g1 <- ggplot(DF,aes(x=obs, fill=Dat, colour=Dat))+geom_bar( position="dodge", aes(group=Dat), colour="black")


g2 <- ggplot(DF,aes(x=Dat, fill=obs, colour=obs))+geom_bar( position="fill", aes(group=obs), colour="black")

plots <- list(g1, g2)

do.call(grid.arrange, plots)
@


<<smote_plot2, fig.cap="Smote strategy applied to ImbC data with different parameters.", echo=FALSE, fig.width=7, fig.height=7>>=

g1 <- ggplot(ImbC, aes(x = X2, y = X1)) + 
  geom_jitter(data = ImbC, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) + 
  ggtitle("ImbC data")

g2 <- ggplot(IC1, aes(x = X2, y = X1)) + 
  geom_jitter(data = IC1, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("IC1 data")

g3 <- ggplot(IC2, aes(x = X2, y = X1)) + 
  geom_jitter(data = IC2, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("IC2 data")

g4 <- ggplot(IC3, aes(x = X2, y = X1)) + 
  geom_jitter(data = IC3, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("IC3 data")

g5 <- ggplot(IC4, aes(x = X2, y = X1)) + 
  geom_jitter(data = IC4, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("IC4 data")


g6 <- ggplot(IC5, aes(x = X2, y = X1)) + 
  geom_jitter(data = IC5, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("IC5 data")

g7 <- ggplot(IC6, aes(x = X2, y = X1)) + 
  geom_jitter(data = IC6, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("IC6 data")

g8 <- ggplot(IC7, aes(x = X2, y = X1)) + 
  geom_jitter(data = IC7, aes(colour=Class, pch=Class)) + 
  scale_color_manual(values=c("lightgreen", "blue", "orange")) +
  scale_shape_manual(values=c(0,16,17)) + 
  ylim(min(ImbC$X1)-1, max(ImbC$X1)+1) +
  ggtitle("IC7 data")

plots <- list(g1,g2,g3,g4,g5,g6,g7,g8,ncol=2)

do.call(grid.arrange, plots)

@


% ====================================================================
\section{Methods for Addressing Utility-based Regression Tasks}\label{sec:methRegres}

Utility-based problems also occur for regression tasks. However, for these problems we have a continuous target variable and therefore are no classes defined. Instead, the user may consider some ranges of the target variable domain more important (which are usually less represented) while other regions of that variable are less important. As proposed by \cite{torgo2007utility, ribeiro2011utility}, utility-based regression problems depend on the definition of a continuous relevance function ($\phi()$) which expresses the importance of the target variable values across its domain. This function $\phi()$, varies between 0 and 1, where 0 represents points in the target variable domain which are not relevant and 1 identifies the most important values. Usually, the user is also asked to provide a relevance threshold (a numeric value in $[0,1]$) which helps to clearly distinguish between the important and unimportant values.

\cite{ribeiro2011utility} proposed a framework for defining the relevance function of a given continuous target variable. This framework has an automatic method that allows to obtain the relevance function from the target variable distribution. The assumption made to achieve this goal regards the most usual setting, where the extreme rare values are the most important to the user. This framework also allows the user to manually specify which are the relevant and irrelevant values using a matrix. The R package \texttt{uba} \cite{uba}, available in \url{http://www.dcc.fc.up.pt/~rpribeiro/uba/}, includes several other functionalities for dealing with utility-based regression. We use in \UBLp the functions regarding the relevance function.


Considering a target variable with domain $[0,10]$, a possible relevance function could be the one represented in Figure \ref{fig:relev_ex}. For this particular regression task, the relevance function selected and the chosen relevance threshold of 0.5 characterize the most important ranges of the target variable and the bumps of relevance. In this case, we have established two bumps which include the most important values (also named ``rare" cases) of the target variable ($[0, 1.5]$ and $[4.5, 7]$ represented in green in Figure~\ref{fig:relev_ex}). On the other hand, the target values falling in the intervals $]1.5, 4.5[$ and $]7,10]$ (represented in red in Figure~\ref{fig:relev_ex}) are the less relevant and ``normal" cases.


<<relev_ex, fig.cap="Example of a relevance function.", fig.width=5, fig.height=5,echo=FALSE,out.width="0.8\\textwidth">>=
  y <- 0:10

   rel <- matrix(0,ncol=3,nrow=0)
   rel <- rbind(rel,c(0,1,0))
   rel <- rbind(rel,c(3,0,0))
   rel <- rbind(rel,c(6,1,0))
   rel <- rbind(rel,c(7,0.5,1))
   rel <- rbind(rel,c(10,0,0))
phiF.args <- phi.control(y,method="range",control.pts=rel)
#y.phi <- phi(y,parms=phiF.args)


yrange <- range(y)
yplot <- seq(yrange[1],yrange[2],len=100)
yplot.phi <- phi(yplot,control.parms=phiF.args)

plot(yplot,yplot.phi,type="l",
     ylab=expression(phi(y)),xlab=expression(y))
abline(h=0.5, lty=2, col=4)

text(x=9, y=0.52, labels="relevance threshold", cex=0.8, col=4)
mtext("0.5",side=2,col=4)
points(1.5,0.5, col=4)
points(4.5,0.5, col=4)
points(7,0.5, col=4)
#abline(h=-0.01,lwd=2, col=2)
lines(c(-0.2,1.5), c(-0.01,-0.01), col="green", lwd=3)
lines(c(1.5,4.5), c(-0.01,-0.01), col="red", lwd=3)
lines(c(4.5,7), c(-0.01,-0.01), col="green", lwd=3)
lines(c(7,10.2), c(-0.01,-0.01), col="red", lwd=3)

lines(c(1.5,1.5), c(0.5,0), col=4, lty=2)
lines(c(4.5,4.5), c(0.5,0), col=4, lty=2)
lines(c(7,7), c(0.5,0), col=4, lty=2)

@


The user has the responsibility of defining a relevance function suitable for the regression task he is considering. We provide the mechanism proposed by \cite{ribeiro2011utility} and also implemented in \texttt{uba} package to assist the user in this task. This method is called \texttt{range}, and depends on the introduction by the user of reference points for the $y$, and corresponding $\phi()$ and $\phi'()$ values. With the method \texttt{range} the relevance function may be manually defined with a 3-column matrix containing the interpolating points as follows:

<<method_range>>=
# relevance function represented in the previous example

## method: range
# the user should provide a matrix with y, phi(y), phi'(y)

rel <- matrix(0, ncol = 3, nrow = 0)

# for the target value of zero the relevance function should be one and
# the derivative at that point should be zero
rel <- rbind(rel, c(0,1,0)) 

# for the value three the relevance assigned is zero and the derivative is zero
rel <- rbind(rel, c(3,0,0))
rel <- rbind(rel, c(6,1,0))
rel <- rbind(rel, c(7,0.5,1))
rel <- rbind(rel, c(10,0,0))
# after defining the relevance function the user may obtain the 
# phi values as follows:

# use method "range" when defining a matrix
phiF.args <- phi.control(y,method = "range", control.pts = rel)

# obtain the relevance values for the target variable y
y.phi <- phi(y,control.parms = phiF.args)

@

In order to facilitate the user task, we also provide an automatic mechanism proposed by \cite{ribeiro2011utility} and also implemented in \texttt{uba} package, for defining the relevance function. This automatic method, called \texttt{extremes} is based on the boxplot of the target variable values and assigns a larger importance to the least represented values. In this case, the user does not need to provide interpolating points because this method assumes that the least represented ranges of the target variable are the most important. We now provide an example of how to use this automatic method.

<<method_extremes>>=

## method: extremes

## for considering only the high extremes
phiF.args <- phi.control(y,method = "extremes",extr.type = "high")
y.phi <- phi(y,control.parms = phiF.args)

## for considering only the low extremes
phiF.args <- phi.control(y,method = "extremes",extr.type = "low")
y.phi <- phi(y,control.parms = phiF.args)

## for considering both extreme types (low and high)
phiF.args <- phi.control(y,method = "extremes",extr.type = "both")
y.phi <- phi(y,control.parms = phiF.args)


@

All the implemented methods for utility-based regression tasks depend on the definition of a relevance function, and the majority of them also rely on a user-defined relevance threshold.


Let us now observe the impact of using the automatic method for defining a relevance function with the synthetic ImbR data provided with \UBLp.

<<Rel_Aut_ImbC>>=
# define that the automatic method will be used and 
# specify that we are only interested in the high extreme values
phiF.args <- phi.control(ImbR$Tgt, method = "extremes", extr.type = "high")
y.phi <- phi(sort(ImbR$Tgt),control.parms = phiF.args)

@

However, the user has also the possibility to define its own relevance function as follows:

<<Rel_User_ImbC>>=
# specify the y, phi(y) and phi'(y) in each row of the matrix
rel <- matrix(c(10, 1, 0, 11, 0, 0, 18, 0.5, 1, 19, 0.8, 0, 21, 1, 0),
              ncol = 3, nrow = 5, byrow = TRUE)
phiF.argsR <- phi.control(ImbR$Tgt, method = "range", control.pts = rel)
y.phiR <- phi(sort(ImbR$Tgt), control.parms = phiF.argsR)
@


Figures~\ref{fig:Rel1} and \ref{fig:Rel2} show the two relevance functions previously obtained for ImbC data (the first one is built with the automatic and the second uses the matrix with interpolating points provided by the user). The automatic method "extremes" takes into account the examples distribution while the "range" method uses the information provided by the user regardless of the domain distribution. In the relevance function specified with "range" method the lower and higher values are both considered extremely relevant.

<<Rel1, echo=FALSE, fig.cap="Relevance function automatically obtained.", fig.width=4, fig.height=4 ,out.width="0.6\\textwidth">>=
y <- sort(ImbR$Tgt)
phiF.argsR <- phi.control(y,method="extremes")
y.phi <- phi(y, control.parms=phiF.argsR)
ggplot(data=data.frame(y,y.phi), aes(x=y, y=y.phi))+geom_line()+xlab("Tgt")+ylab(expression(phi()))

@


<<Rel2, echo=FALSE, fig.cap="Relevance function obtained through a matrix with interpolating points provided by the user.", fig.width=4, fig.height=4, out.width="0.6\\textwidth">>=

y <- sort(ImbR$Tgt)
phiF.argsR <- phi.control(y,method="range",control.pts=rel)
y.phi <- phi(y, control.parms=phiF.argsR)
ggplot(data=data.frame(y,y.phi), aes(x=y, y=y.phi))+geom_line()+xlab("Tgt")+ylab(expression(phi()))

@

In the next sections we describe the following methods for tackling utility-based regression tasks:
\begin{itemize}
\item \ref{sec:RURegress} Random Under-sampling
\item \ref{sec:RORegress} Random Over-sampling
\item \ref{sec:gnRegress} Gaussian Noise Introduction
\item \ref{sec:smoteR} SmoteR Algorithm
\item \ref{sec:ISRegress} Importance Sampling

\end{itemize}

\subsection{Random Under-sampling}\label{sec:RURegress}

Random under-sampling strategy for regression problems was first proposed by \cite{torgo2013smote}. This strategy is similar to the strategy presented for classification. It depends on the definition of both a relevance function and a relevance threshold. In this proposal, all the target values below the relevance threshold are considered normal and uninteresting and thus are regarded as candidates to be under-sampled. The user is also asked to set another parameter that establishes the proportion between normal (unimportant) and rare (important) cases that the new under-sampled data set should contain.


In the implementation of this strategy in \pUBL, we ask for the user to define the relevance function (manually through the method ``range" or using the automatic method, called ``extremes", previously described). This means that the user may define as many relevance bumps as wanted. Parameter \texttt{rel} is used to indicate the relevance function. For using the automatic method the parameter \texttt{rel} should be set to ``auto" (the default). If the user wants to apply the range method, then, as previously explained, a 3-column matrix should be provided. It is also necessary for the user to define a relevance threshold through the \texttt{thr.rel} parameter. Having this set, all the target variable values with relevance below the relevance threshold are candidates to be under-sampled. Finally, the user can also express using the \texttt{C.perc} parameter which under-sampling percentage should be applied in each bump with uninteresting values, or alternatively this parameter may be set to ``balance" or ``extreme". If ``balance" is chosen the under-sampling percentage is automatically estimated in order to balance the normal/important and rare/unimportant cases. On the other hand, the ``extreme" option will invert the existing frequencies. The following example uses the regression data set provided with \UBLp, ImbR, to show how these parameters can be set and their impact on the changed data.



<<ImbR_RU_1>>=
data(ImbR) # load the synthetic data set provided with UBL

# Using the automatic method for defining the relevance function
# This is the default behaviour, therefore, we can simply 
# not mention the "rel" parameter

# default of C.perc parameter balances the examples in the bumps
IRU1 <- RandUnderRegress(Tgt~., ImbR)

IRU2 <- RandUnderRegress(Tgt~., ImbR, C.perc = "extreme")

# the automatic method for the relevance function generates only 
# one bump with uninteresting values, thus we only need to set 
# one under-sampling percentage
IRU3 <- RandUnderRegress(Tgt~., ImbR, C.perc = list(0.5))

@


Figure~\ref{fig:ImbR_RU_ex1} shows the impact of the applied strategies on the density of target variable of ImbR data.
<<ImbR_RU_ex1, fig.cap="Automatic relevance function and density of the target variable in the original ImbR data and the changed data sets using Random Under-sampling strategy", echo=FALSE, out.width="0.8\\textwidth">>=
par(mar = c(5,5,2,5))
plot(density(ImbR$Tgt), xlab=expression(Tgt), main="")
lines(density(IRU1$Tgt), col=3)
lines(density(IRU2$Tgt), col=4)
lines(density(IRU3$Tgt), col=6)

y <- sort(ImbR$Tgt)

pc <- phi.control(y, method="extremes")
y.relev <- phi(y,pc)

par(new=TRUE)
plot(y,y.relev, lty=2, col="grey", lwd=3, axes=F, ylab=NA, xlab=NA,type="l")
axis(4, xaxt="n",col ="grey", col.axis ="grey" , lwd = 2)
mtext(expression(phi(y)),4, col="grey", line=3)
legend(20,0.9, c("ImbR", "IRU1", "IRU2", "IRU3", expression(phi())), col=c(1,3,4,6,"grey"), lty=c(1,1,1,1,2), lwd=c(1,1,1,1,2), bty="n", text.col=c(1,1,1,1,"grey"))
@

We can also observe the impact of the previously defined changes on the examples distribution in Figure~\ref{fig:ImbR_RU_dist}.


<<ImbR_RU_dist, echo=FALSE, fig.height=6.5, fig.width=8, fig.cap="Original and changed data sets using different parameters of the random under-sampling strategy.">>=

g1 <- ggplot(ImbR, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = ImbR, aes(colour=Tgt)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("Original ImbR data")

g2 <- ggplot(IRU1, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = IRU1, aes(colour=Tgt)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("IRU1 data")
  
g3 <- ggplot(IRU2, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = IRU2, aes(colour=Tgt)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("IRU2 data")

g4 <- ggplot(IRU3, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = IRU3, aes(colour=Tgt)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("IRU3 data")

plots <- list(g1, g2, g3, g4, ncol=2)

do.call(grid.arrange, plots)


@


Let us now assume that we have some domain knowledge that leads us to considered a different relevance function. Suppose that the most relevant cases are the target varaible values close to 15. In the following example we define a new relvance function suitable for this context and apply the random under-sampling strategy to change the original data set with different parameters.

<<ImbR_user_RU>>=

rel <- matrix(c(14, 0, 0, 15, 1, 0, 16, 0, 0, 20, 1, 0, 21, 0, 0),
              ncol=3, nrow=5, byrow=TRUE)
dsU1 <- RandUnderRegress(Tgt~., ImbR, rel=rel)
dsU2 <- RandUnderRegress(Tgt~., ImbR, rel=rel, C.perc=list(0.5))
dsU3 <- RandUnderRegress(Tgt~., ImbR, rel=rel, C.perc=list(0.2))

@


Figures~\ref{fig:I_u_RU1} and \ref{fig:I_u_RU2} show the density of the target variable in the original ImbR data and the pre-processed data sets and the examples distribution.


<<I_u_RU1, fig.cap="Density of the target variable in the original ImbR data and the changed data sets using Random Under-sampling strategy and a user defined relevance function.", echo=FALSE, out.width="0.8\\textwidth">>=
par(mar = c(5,5,2,5))
plot(density(ImbR$Tgt), main="", xlim=c(9.5,24), xlab=expression(Tgt))
lines(density(dsU1$Tgt), col=3, xlim=c(9.5,24))
lines(density(dsU2$Tgt), col=4, xlim=c(9.5,24))
lines(density(dsU3$Tgt), col=6, xlim=c(9.5,24))

y <- sort(ImbR$Tgt)

rel <- matrix(c(14, 0, 0, 15, 1, 0, 16, 0, 0, 20, 1, 0, 21, 0, 0), ncol=3, nrow=5, byrow=TRUE)

pc <- phi.control(y, method="range", control.pts=rel)
y.relev <- phi(y,pc)

par(new=TRUE)
plot(y,y.relev, lty=2, col="grey", lwd=3, axes=F, ylab=NA, xlab=NA,type="l")
axis(4, xaxt="n",col ="grey", col.axis ="grey" , lwd = 2)
mtext(expression(phi(y)),4, col="grey", line=3)
legend(20,0.9, c("ImbR", "dsU1", "dsU2", "dsU3", expression(phi())), col=c(1,3,4,6,"grey"), lty=c(1,1,1,1,2), lwd=c(1,1,1,1,2), bty="n", text.col=c(1,1,1,1,"grey"))
@


<<I_u_RU2, echo=FALSE, fig.height=6.5, fig.width=8, fig.cap="Original and changed data sets using different parameters of the random under-sampling strategy and a user defined relevance function.">>=

g1 <- ggplot(ImbR, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = ImbR, aes(colour=Tgt)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("Original ImbR data")

g2 <- ggplot(dsU1, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = dsU1, aes(colour=Tgt)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("dsU1 data")
  
g3 <- ggplot(dsU2, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = dsU2, aes(colour=Tgt)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("dsU2 data")

g4 <- ggplot(dsU3, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = dsU3, aes(colour=Tgt)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("dsU3 data")

plots <- list(g1, g2, g3, g4, ncol=2)

do.call(grid.arrange, plots)


@


We must highlight that this strategy entails some consequences that should not be disregarded. Namely, there can be a sever impact on the total number of examples in the modified data sets. If we are considering a large data set, possibly removing 100 points may have a negligible impact. However, if the data set is already small, then removing 100 examples may have an huge impact. 

This can be observed in the previous examples. In fact, the \texttt{C.perc} parameter must be thought carefully due to the consequences on the total number of examples. In Table \ref{tab:RUReg_table} we can check the impact of the several strategies on the data set for the two relevance functions considered (the obtained through the automatic method and the defined with a 3-column matrix).

<<RU_table, echo=FALSE, results='asis' >>=

nm <- list(ImbR, IRU1, IRU2, IRU3, dsU1, dsU2, dsU3)
names <- c("ImbR", "IRU1", "IRU2", "IRU3", "dsU1", "dsU2", "dsU3")
m <- matrix(sapply(nm, nrow),nrow=1, ncol=7, dimnames=list("nr. examples", names))

xt <-xtable(m, caption="Total number of examples in each data set for different parameters of random under-sampling strategy.",label="tab:RUReg_table")

print(xt,comment = FALSE, type = 'latex')
@



\subsection{Random Over-sampling}\label{sec:RORegress}

The Random over-sampling method proposed is an adaptation of the Random over-sampling method proposed for classification tasks using the previously presented relevance function for utility-based regression tasks. This technique is available through \texttt{randomOverRegress} function, and is simply based on the random introduction of replicas of examples of the original data set. These replicas are only introduced in the most important ranges of the target variable, i.e., in the ranges where the relevance is above a user-defined threshold. Similarly to what happened in Random under-sampling, the user may define its own relevance function or use the automatic method provided to generate one. It is also the user responsibility to define the relevance threshold (using the \texttt{thr.rel} parameter) and the percentages of over-sampling to apply in each bump of relevance (through the \texttt{C.perc} parameter). Alternatively, the user may set the \texttt{C.perc} parameter as ``balance" or ``extreme", cases which automatically evaluate the percentages of over-sampling to apply for obtaining a new balanced data set or for inverting the frequencies of examples in the defined bumps.
In the following example we can see how to use this function.

<<RO_autoRel>>=
# using the automatic method for defining the relevance function and
# the default threshold of 0.5
IRO <- RandOverRegress(Tgt~., ImbR, C.perc=list(2.5))
IROBal <- RandOverRegress(Tgt~., ImbR, C.perc="balance")
IROExt <- RandOverRegress(Tgt~., ImbR, C.perc="extreme")

# change the relevance threshold to 0.9
IRO0.9 <- RandOverRegress(Tgt~., ImbR, thr.rel=0.9)
@


Figure \ref{fig:RO_ex1} shows the impact of this method for several parameters.

<<RO_ex1, fig.cap="Relevance function and density of the target variable in the original and new data sets using Random over-sampling strategy.", echo=FALSE>>=
par(mar = c(5,5,2,5))
plot(density(ImbR$Tgt), xlab=expression(Tgt), main="")
lines(density(IRO$Tgt), col=3)
lines(density(IROBal$Tgt), col=4)
lines(density(IROExt$Tgt), col=6)
lines(density(IRO0.9$Tgt), col=7)

y <- sort(ImbR$Tgt)

pc <- phi.control(y, method="extremes")
y.relev <- phi(y,pc)

par(new=TRUE)
plot(y,y.relev, lty=2, col="grey", lwd=3, axes=F, ylab=NA, xlab=NA,type="l")
axis(4, xaxt="n",col = "grey", col.axis = "grey" , lwd = 2)
mtext(expression(phi(y)),4, col="grey", line=3)
legend(20,0.9, c("ImbR", "IRO", "IROBal", "IROExt", "IRO0.9",expression(phi())), col=c(1,3,4,6,7,"grey"), lty=c(1,1,1,1,1,2), lwd=c(1,1,1,1,1,2), bty="n", text.col=c(1,1,1,1,1,"grey"))
@

This method also carries a strong impact on the total number of examples in the modified data set. While the random Under-sampling method is able to produce a significant reduction of the data set, the random over-sampling technique will increase, sometimes drastically, the data set size. Table \ref{tab:ROReg_table} shows the impact of the previous examples on the total number of examples of used the data set.

<<RO_table, echo=FALSE, results='asis' >>=

nm <- list(ImbR, IRO,  IROBal, IROExt, IRO0.9)
names <- c("ImbR", "IRO", "IROBal", "IROExt", "IRO0.9")
m <- matrix(sapply(nm, nrow), nrow=1, ncol=5, dimnames=list("nr. examples", names))

xt <-xtable(m, caption="Total number of examples in each data set for different parameters of random over-sampling strategy.",label="tab:ROReg_table")

print(xt,comment = FALSE, type = 'latex')
@

As expected, all the data sets have an increased size. However, for the \texttt{IRO0.9} data set, the size was increased approximately \Sexpr{round(nrow(IRO0.9)*100/nrow(ImbR),0)}\%. This ``side effect" must be taken into consideration when applying this technique because it may impose constraints on the used learners. We must also highlight that, although the data set size can be strongly increased, we are in fact only introducing replicas of already existing examples, and thus no new information is being inserted.



\subsection{Generation of synthetic examples by the introduction of Gaussian Noise}\label{sec:gnRegress}

The generation of synthetic examples through the introduction of small perturbations based on Gaussian Noise was a strategy proposed for classification tasks \cite{lee1999regularization, lee2000noisy}. The main idea of this strategy is to generate new synthetic examples with a desired class label, by perturbing the features of examples of that class a certain amount of the respective standard deviation. 

We have adapted this over-sampling technique to regression problems and have combined it with the random under-sampling method. To accomplish this it is required that the user defines a relevance function and a relevance threshold. The examples which have a target variable value with relevance higher than the threshold set will be over-sampled, and the remaining will be randomly under-sampled. The under-sampling strategy used is the same described in Section~\ref{sec:RURegress}. For the over-sampling strategy we use the same procedure which was described for classification tasks in Section~\ref{sec:gnClassif}. The only difference on the over-sampling method is in the target variable value. For classification tasks, the target variable value was easily assigned: it was the rare class under consideration. For regression tasks we have decided to extend the technique applied for numeric features also to the target variable. This means that the new example target variable value is obtained by a random normal perturbation of the original target value based on the target value standard deviation.

In order to use this method the user must provide a relevance function through the \texttt{rel} parameter (or use the automatic method for estimating it by setting rel to ``auto"), a threshold on the relevance (parameter \texttt{thr.rel}) and the perturbation to be used (parameter \texttt{pert}). Moreover, the user may also express using the parameter \texttt{C.perc} the percentages of over and under-sampling to apply in each bump defined, or alternatively he may set this parameter to ``balance" or ``extreme". Similarly to the behavior described in the previous techniques, setting this parameter to ``balance" or ``extreme" causes the percentages of over and under-sampling to be automatically estimated. The option ``balance" will try to distribute the examples evenly across the existing bumps while maintaining the total number of examples in the modified data set. If the choice is ``extreme" then the frequencies of the examples in the bumps will be inverted. The user can also indicate if the under-sampling process can be made with repetition of examples or not using the \texttt{repl} parameter.

We now show some examples of usage of the function \texttt{GaussNoiseRegress}.
<<GN_1>>=
# relevance function estimated automatically has two bumps
# defining the desired percentages of under and over-sampling to apply
C.perc=list(0.5, 3)
# define the relevance threshold
thr.rel=0.8
mygn <- GaussNoiseRegress(Tgt~., ImbR, thr.rel=thr.rel, C.perc=C.perc)
gnB <- GaussNoiseRegress(Tgt~., ImbR, thr.rel=thr.rel, C.perc="balance")
gnE <- GaussNoiseRegress(Tgt~., ImbR, thr.rel=thr.rel, C.perc="extreme")

@

Figures~\ref{fig:GN_plot1} and \ref{fig:GN_plot2} show the impact of this strategy, for the parameters considered, on the examples distribution. In Figure~\ref{fig:GN_plot2} we have binarized the data sets into rare/important ($+$) and normal/unimportant cases ($-$). We have considered the threshold of 19 on the target varaible to distinguish between the two "classes". Figure~\ref{fig:GN_plot_new} shows the true distribution of the target variable in the original ImbR data and the pre-processed data sets.

<<GN_plot1, fig.cap="Relevance function and density of the target variable in the original and new data sets using Gaussian noise strategy.", echo=FALSE>>=
par(mar = c(5,5,2,5))
plot(density(ImbR$Tgt), xlab=expression(Tgt), main="")
lines(density(mygn$Tgt), col=3)
lines(density(gnB$Tgt), col=4)
lines(density(gnE$Tgt), col=6)

names <- c("ImbR", "mygn", "gnB", "gnE")
y <- sort(ImbR$Tgt)
pc <- phi.control(y, method="extremes")
y.relev <- phi(y,pc)

par(new=TRUE)
plot(y,y.relev, lty=2, col="grey", lwd=2, axes=F, ylab=NA, xlab=NA,type="l")
axis(4, xaxt="n",col ="grey", col.axis ="grey" , lwd = 2)
mtext(expression(phi(Tgt)),4, col="grey", line=3)
legend(20,0.9, c("ImbR", "mygn", "gnB", "gnE",expression(phi())), col=c(1,3,4,6,"grey"), lty=c(1,1,1,1,2), lwd=c(1,1,1,1,2), bty="n", text.col=c(1,1,1,1,"grey"))
@




<<GN_plot2,fig.cap="The impact of Gaussian Noise strategy in a binarized version of ImbR data considering Tgt values above 19 as rare and below 19 as normal cases.",out.width="0.8\\textwidth", echo=FALSE, warning=FALSE, fig.height=8, fig.width=8>>=
par(mfrow = c(2, 2))
plot(ImbR$X1, ImbR$X2, col= ifelse(ImbR$Tgt>19,2,3), pch=ifelse(ImbR$Tgt>19,"+", "-"), main="Original data", xlim=c(-1,22), ylim=c(-1,22), xlab="X1", ylab="X2")
par(xpd=TRUE)
legend("topright", inset=c(0,0),col=c(2,3), c("rare cases", "normal cases"),pch=c("+", "-"), bty="n",cex=0.7)
plot(mygn$X1, mygn$X2, col= ifelse(mygn$Tgt>19,2,3), pch=ifelse(mygn$Tgt>19,"+", "-"), main="user specified percentages", xlim=c(-1,22), ylim=c(-1,22), xlab="X1", ylab="X2")
plot(gnB$X1, gnB$X2, col= ifelse(gnB$Tgt>19,2,3),pch=ifelse(gnB$Tgt>19,"+", "-"), main="balance method", xlim=c(-1,22), ylim=c(-1,22), xlab="X1", ylab="X2")
plot(gnE$X1, gnE$X2, col= ifelse(gnE$Tgt>19,2,3), pch=ifelse(gnE$Tgt>19,"+", "-"), main="extreme method", xlim=c(-1,22), ylim=c(-1,22), xlab="X1", ylab="X2")

@

<<GN_plot_new,fig.cap="Target variable distribution on ImbR and data sets changed through Gaussian Noise strategy.", fig.height=6.5, fig.width=8, echo=FALSE>>=

g1 <- ggplot(ImbR, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = ImbR, aes(colour=Tgt)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("Original ImbR data")

g2 <- ggplot(mygn, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = mygn, aes(colour=Tgt)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("mygn data")
  
g3 <- ggplot(gnB, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = gnB, aes(colour=Tgt)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("gnB data")

g4 <- ggplot(gnE, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = gnE, aes(colour=Tgt)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("gnE data")

plots <- list(g1, g2, g3, g4, ncol=2)

do.call(grid.arrange, plots)

@

In the following example we check the impact of changing the perturbation introduced.

<<gn_balance_pert_eval>>=
# the default uses the value of 0.1 for "pert" parameter
gnB1 <- GaussNoiseRegress(Tgt~., ImbR, thr.rel=thr.rel, C.perc="balance")

# try two different values for "pert" parameter
gnB2 <- GaussNoiseRegress(Tgt~., ImbR, thr.rel=thr.rel, C.perc="balance",
                          pert=0.5)
gnB3 <- GaussNoiseRegress(Tgt~., ImbR, thr.rel=thr.rel, C.perc="balance",
                          pert=0.01)

@



The impact of changing the parameter \texttt{pert} is represented in Figures~\ref{fig:GN_plot3_dist} and \ref{fig:GN_plot3}.
<<GN_plot3,fig.cap="Impact of changing the pert parameter in Gaussian Noise strategy in a binarized version of ImbR data.",out.width="0.8\\textwidth", echo=FALSE>>=
par(mfrow = c(2, 2))
plot(ImbR$X1, ImbR$X2, col= ifelse(ImbR$Tgt>19,2,3), pch=ifelse(ImbR$Tgt>19,"+", "-"), main="Original data", xlim=c(-1,22), ylim=c(-1,22), xlab="X1", ylab="X2")
plot(gnB1$X1, gnB1$X2, col= ifelse(gnB1$Tgt>19,2,3), pch=ifelse(gnB1$Tgt>19,"+", "-"), main="pert=0.1", xlim=c(-1,22), ylim=c(-1,22), xlab="X1", ylab="X2")
par(xpd=TRUE)
legend("topright", inset=c(0,0),col=c(2,3), c("rare cases", "normal cases"),pch=c("+", "-"), bty="n")
plot(gnB2$X1, gnB2$X2, col= ifelse(gnB2$Tgt>19,2,3),pch=ifelse(gnB2$Tgt>19,"+", "-"), main="pert=0.5", xlim=c(-1,22), ylim=c(-1,22), xlab="X1", ylab="X2")
plot(gnB3$X1, gnB3$X2, col= ifelse(gnB3$Tgt>19,2,3), pch=ifelse(gnB3$Tgt>19,"+", "-"), main="pert=0.01", xlim=c(-1,22), ylim=c(-1,22), xlab="X1", ylab="X2")

@

<<GN_plot3_dist,fig.cap="Target variable distribution on ImbR and data sets changed through Gaussian Noise strategy.", fig.height=6.5, fig.width=8, echo=FALSE, warning=FALSE>>=

g1 <- ggplot(ImbR, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = ImbR, aes(colour=Tgt)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("Original ImbR data")

g2 <- ggplot(gnB1, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = gnB1, aes(colour=Tgt)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("gnB1 data")
  
g3 <- ggplot(gnB2, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = gnB2, aes(colour=Tgt)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("gnB2 data")

g4 <- ggplot(gnB3, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = gnB3, aes(colour=Tgt)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("gnB3 data")

plots <- list(g1, g2, g3, g4, ncol=2)

do.call(grid.arrange, plots)

@


\subsection{The SmoteR Algorithm}\label{sec:smoteR}

The SmoteR algorithm was presented in \cite{torgo2013smote}. This proposal is an adaptation for regression problems under imbalanced domains of the existing smote algorithm \cite{CBOK02} for classification tasks. As with other methods addressing regression tasks on imbalanced data distributions it is the user responsability to provide a relevance function and a relevance threshold. This function determines which are the relevant and the unimportant examples. This algorithm combines an over-sampling strategy by interpolation of examples with a random under-sampling approach. For the generation of new examples by interpolation, the same procedure proposed in smote algorithm is used. Regarding the generation of the target variable value of the new generated examples the proposed smoteR algorithm uses an weighted average of the values of target variable of the two examples used. The weights are calculated as an inverse function of the distance of the generated case to each of the two seed examples. This means that, the further away the new example is from the seed case less weight will be given for the generation of the target variable value. The random under-sampling approach is applied in the bumps containing the normal and unimportant cases. 

The smoteR algorithm is available through the \texttt{SmoteRegress} function. The user may define its own relevance function or use the automatic method, as in the previously described techniques. The user must also define the relevance threshold. 
Regarding the generation of examples it is required to specify the number of nearest neighbors to consider in smoteR algorithm. This is available through the parameter \texttt{k} and the default is set to 5. The user may then use the \texttt{C.perc} parameter to either express the percentages of under and over-sampling to use in each bump of relevance or to set which automatic method should be used for determining these percentages. Similarly to the other approaches, the automatic methods available are ``balance" and ``extreme" which estimate both where to apply the under/over-sampling and the corresponding percentages. The method ``balance" changes the examples distribution by assigning roughly the same number of examples to each bump while the ``extreme" method inverts the frequencies of each bump. Both methods approximately maintain the total number of examples. The parameter \texttt{repl} allows to select if the random under-sampling strategy is applied with repetition of examples or not. The user can also specify which distance function should be used for the nearest neighbors computation using the \texttt{dist} parameter. 

The following examples illustrate how this method can be used.

<<smoteR_rel1, fig.cap="Relevance function obtained automatically for the ImbR data set", fig.height=8, fig.width=8,out.width="0.8\\textwidth">>=

# we will use the automatic method for defining the relevance function and will
# set the relevance threshold to 0.8 
# this method splits the data set in two: a first range of values normal and less
# important and a second range with the interesting cases

# to check this, we can plot the relevance function obtained automatically
# as follows:

y <- sort(ImbR$Tgt)
phiF.args <- phi.control(y, method = "extremes", extr.type = "both")
y.phi <- phi(y, control.parms = phiF.args)

# plot the relevance function
plot(y, y.phi, type="l",
     ylab = expression(phi(y)), xlab = expression(y))

#add the relevance threshold to the plot
abline(h = 0.8, col = 3, lty = 2)
@

Figure~\ref{fig:smoteR_rel1} shows that we are considering two different bumps: a first bump with the normal and less important cases and a second bump with the rare and interesting cases. Thus, to address this problem we can do the following:

<<smoteR_ex1>>=
# we have two bumps: the first must be under-sampled and the second over-sampled. 
# Thus, we can chose the following percentages: 
thr.rel = 0.8
C.perc = list(0.1, 8)

# using these percentages and the relevance threshold of 0.8 with all
# the other parameters default values
# we can select any distance function 
# because the data set contains only numeric features
mysm <- SmoteRegress(Tgt~., ImbR, thr.rel=thr.rel, dist="Manhattan", C.perc=C.perc)

# use the automatic method for obtaining a balanced data set
smB <- SmoteRegress(Tgt~., ImbR, thr.rel=thr.rel, dist="Manhattan", C.perc="balance")

# use the automatic method for invert the frequencies of the bumps
smE <- SmoteRegress(Tgt~., ImbR, thr.rel=thr.rel, dist="Manhattan", C.perc="extreme")

@

This strategy changes the examples distribution as shown in Figure~\ref{fig:smoteR_plot1}.

<<smoteR_plot1, fig.cap="Relevance function and density of the target variable in the original and new data sets using smoteR strategy.", echo=FALSE>>=
par(mar = c(5,5,2,5))
plot(density(ImbR$Tgt), xlab=expression(Tgt), main="")
lines(density(mysm$Tgt), col=3)
lines(density(smB$Tgt), col=4)
lines(density(smE$Tgt), col=6)

names <- c("ImbR", "mysm", "smB", "smE")
y <- sort(ImbR$Tgt)
pc <- phi.control(y, method = "extremes")
y.relev <- phi(y, pc)

par(new=TRUE)
plot(y,y.relev, lty=2, col="grey", lwd=1, axes=F, ylab=NA, xlab=NA,type="l")
axis(4, xaxt="n",col ="grey", col.axis ="grey" , lwd = 1)
mtext(expression(phi()),4, col="grey", line=3)
legend(20,0.9, c("ImbR", "mysm", "smB", "smE",expression(phi())), col=c(1,3,4,6,"grey"), lty=c(1,1,1,1,2), lwd=c(1,1,1,1,1), bty="n", text.col=c(1,1,1,1,"grey"))
@

Figure~\ref{fig:SMOTE_plot3_dist} shows the Original ImbR data and the new data sets pre-processed with SmoteR strategy.

<<SMOTE_plot3_dist,fig.cap="Target variable distribution on ImbR and data sets changed through SmoteR strategy.", fig.height=6.5, fig.width=8, echo=FALSE, warning=FALSE>>=

g1 <- ggplot(ImbR, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = ImbR, aes(colour=Tgt)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("Original ImbR data")

g2 <- ggplot(mysm, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = mysm, aes(colour=Tgt)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("mysm data")
  
g3 <- ggplot(smB, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = smB, aes(colour=Tgt)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("smB data")

g4 <- ggplot(smE, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = smE, aes(colour=Tgt)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("smE data")

plots <- list(g1, g2, g3, g4, ncol=2)

do.call(grid.arrange, plots)

@


We can also obtain the number of examples that each bump contains. Table~\ref{tab:smoteR_1} shows the examples distribution for the considered strategies.

<<smoteR_count1, echo=FALSE>>=
# check the number of examples in each bump of relevance
y <- sort(ImbR$Tgt)
pc <- phi.control(y, method="extremes")
y.relev <- phi(y,pc)

y.rel.mysm <- phi(mysm$Tgt,pc)
y.rel.smB <- phi(smB$Tgt,pc)
y.rel.smE <- phi(smE$Tgt,pc)

imp <-list(y.relev, y.rel.mysm, y.rel.smB, y.rel.smE)
res <- sapply(imp, function(x)c(length(which(x<thr.rel)), length(which(x>=thr.rel))))

m <- matrix(t(res), nrow=4, ncol=2, dimnames=list(names, c("first bump", "second bump")))
@


<<smoteR_1, echo=FALSE, results='asis'>>=
xt <-xtable(m, caption="Number of examples in each bump of relevance for different parameters of smoteR strategy.",label="tab:smoteR_1")
print(xt,comment = FALSE, type = 'latex')

@

In Figure~\ref{fig:smoteR_1bar} we can visualize the impact of these approaches on the examples distribution for each bump of relevance.

<<smoteR_1bar, echo=FALSE, fig.cap="Impact in the distribution of examples for several parameters in smoteR strategy. ",out.height="0.5\\textheight">>=
DF <- rbind(data.frame(Dat="Original", obs=c(rep("first.bump",res[1]), rep("second.bump", res[2]))), data.frame(Dat="mysm", obs=c(rep("first.bump",res[3]), rep("second.bump", res[4]))), data.frame(Dat="smB", obs=c(rep("first.bump",res[5]), rep("second.bump", res[6]))), data.frame(Dat="smE", obs=c(rep("first.bump",res[7]), rep("second.bump", res[8]))))

g1 <- ggplot(DF,aes(x=obs, fill=Dat, colour=Dat))+geom_bar(position="dodge", aes(group=Dat), colour="black")

g2 <- ggplot(DF,aes(x=Dat, fill=obs, colour=obs))+geom_bar(position="fill", aes(group=obs), colour="black")

plots <- list(g1, g2)

do.call(grid.arrange, plots)

@



\subsection{Importance Sampling}\label{sec:ISRegress}

The Importance Sampling method is a new proposal whose main idea is to use the relevance function ($\phi()$) defined for a regression problem as a probability for resampling the examples combining over with under-sampling. This method simply removes some of the examples and includes in the data set replicas of other existing examples. There is no generation of new synthetic examples. For the over-sampling strategy, replicas of examples are introduced by selecting examples according to the relevance function defined, i.e., the higher the relevance of an example, the higher is the probability of being selected as a new replica to include. The under-sampling strategy selects examples to remove according to the function $1-\phi(y)$, i.e, the higher the relevance value of an example, the lower will be the probability of removing it.

This method includes two main behaviors which can be distinguished by the definition or not of a threshold on the relevance function. This means that, if the user decides to chose a relevance threshold the strategy will take this value into consideration with under and over-sampling being applied only on the defined bumps. However, if the user decides not to set a threshold on the relevance then over sampling and under-sampling strategies will also be applied but without a strict bound, i.e., there may be regions of the target variable values where under-sampling and over-sampling are performed together.

The strategy that depends on the definition of a relevance threshold, has the relevance bumps well defined. For these bumps, the user has several alternatives available through the \texttt{C.perc} parameter: the percentages of over and under-sampling to apply may be explicitly defined, or one of the options ``balance" or ``extreme" may be chosen. These last two option for the \texttt{C.perc} parameter allow to estimate the under and over-sampling percentages automatically. The option ``balance" allows to obtain  a balanced data set across the different existing bumps. The ``extreme" option will produce a new data set with the examples frequencies in the bumps inverted. In this setting, there is no range of the target variable where both under and over-sampling techniques are applied. 

As previously mentioned, there is the possibility of not defining a relevance threshold, and simply use the relevance function to decide which examples should be replicated and which should be removed. In this case, the user does not set a threshold on the relevance, but he can define the importance that over and under-sampling should have. In this case, the \texttt{C.perc} parameter is ignored and two other parameters(\texttt{U} and \texttt{O}) are considered instead. The parameters \texttt{U} and \texttt{O} allow the user to define (in a $[0,1] scale$) the importance that the under/over-sampling have, i.e, these parameters assign a weight to the two methods. The higher is \texttt{O} parameter, the higher is the number of replicas selected. In a similar way, the higher is \texttt{U} parameter the higher is the number of examples removed.

The function \texttt{ImpSampRegress} allows the use of Importance Sampling strategy. Some examples on how to use this approach are provided next.

<<IS_1>>=
# relevance function estimated automatically has two bumps
# using the strategy with threshold definition
C.perc=list(0.2,6)
myIS <- ImpSampRegress(Tgt~., ImbR, thr.rel=0.8,C.perc=C.perc)
ISB <- ImpSampRegress(Tgt~., ImbR, thr.rel=0.8, C.perc="balance")
ISE <- ImpSampRegress(Tgt~., ImbR, thr.rel=0.8, C.perc="extreme")

@

Figures \ref{fig:IS_plot1} and \ref{fig:IS_new_plot3} show the impact on the density and distribution of the examples for the new data sets obtained with Importance Sampling strategy. Figure~\ref{fig:IS_plot2} shows a binarized version of the previous data sets considering the value 19 as the threshold between the rare/important cases and the normal/unimportant cases. 

<<IS_plot1, fig.cap="Relevance function and density of the target variable in the original and new data sets using Importance Sampling strategy.", echo=FALSE>>=
par(mar = c(5,5,2,5))
plot(density(ImbR$Tgt), xlab=expression(Tgt), main="")
lines(density(myIS$Tgt), col=3)
lines(density(ISB$Tgt), col=4)
lines(density(ISE$Tgt), col=6)

names <- c("ImbR", "myIS", "ISB", "ISE")
y <- sort(ImbR$Tgt)
pc <- phi.control(y, method="extremes")
y.relev <- phi(y,pc)

par(new=TRUE)
plot(y,y.relev, lty=2, col="grey", lwd=1, axes=F, ylab=NA, xlab=NA,type="l")
axis(4, xaxt="n",col ="grey", col.axis ="grey" , lwd = 1)
mtext(expression(phi(y)),4, col="grey", line=3)
legend(20,0.9, c(names,expression(phi())), col=c(1,3,4,6,"grey"), lty=c(1,1,1,1,2), lwd=c(1,1,1,1,1), bty="n", text.col=c(1,1,1,1,"grey"))
@

<<IS_new_plot3,fig.cap="Target variable distribution on ImbR and data sets changed through Importance Sampling strategy.", fig.height=6.5, fig.width=8, echo=FALSE, warning=FALSE>>=

g1 <- ggplot(ImbR, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = ImbR, aes(colour=Tgt)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("Original ImbR data")

g2 <- ggplot(myIS, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = myIS, aes(colour=Tgt),position=position_jitter(width=.5, height=.5)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("myIS data")
  
g3 <- ggplot(ISB, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = ISB, aes(colour=Tgt),position=position_jitter(width=.5, height=.5)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("ISB data")

g4 <- ggplot(ISE, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = ISE, aes(colour=Tgt),position=position_jitter(width=.5, height=.5)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("ISE data")

plots <- list(g1, g2, g3, g4, ncol=2)

do.call(grid.arrange, plots)

@


<<IS_plot2,fig.cap="Impact of Importance Sampling strategy in a binarized version of the data sets considering the value 19 as the threshold between rare and normal cases.",out.width="0.8\\textwidth", echo=FALSE>>=
par(mfrow = c(2, 2))
plot(ImbR$X1, ImbR$X2, col= ifelse(ImbR$Tgt>19,2,3), pch=ifelse(ImbR$Tgt>19,"+", "-"), main="Original data", xlim=c(-1,22), ylim=c(-1,22), xlab="X1", ylab="X2")
plot(jitter(myIS$X1), jitter(myIS$X2), col= ifelse(myIS$Tgt>19,2,3), pch=ifelse(myIS$Tgt>19,"+", "-"), main="User-defined percentages", xlim=c(-1,22), ylim=c(-1,22), xlab="X1", ylab="X2")
par(xpd=TRUE)
legend("topright", inset=c(0,0),col=c(2,3), c("important/rare cases", "normal/uninteresting cases"),pch=c("+", "-"), bty="n")
plot(jitter(ISB$X1), jitter(ISB$X2), col= ifelse(ISB$Tgt>19,2,3),pch=ifelse(ISB$Tgt>19,"+", "-"), main="balance", xlim=c(-1,22), ylim=c(-1,22), xlab="X1", ylab="X2")
plot(jitter(ISE$X1), jitter(ISE$X2), col= ifelse(ISE$Tgt>19,2,3), pch=ifelse(ISE$Tgt>19,"+", "-"), main="extreme", xlim=c(-1,22), ylim=c(-1,22), xlab="X1", ylab="X2")

@

We now provide some examples of the use of this strategy without the definition of a relevance threshold.

<<IS_2>>=
# relevance function is also estimated automatically
# the default is not to use a relevance threshold and to assign equal 
# importance to under and over-sampling, i.e., U=0.5 and O=0.5
ISD <- ImpSampRegress(Tgt~., ImbR) 
IS1 <- ImpSampRegress(Tgt~., ImbR, U=0.9, O=0.2)
IS2 <- ImpSampRegress(Tgt~., ImbR, U=0.5, O=0.8)

@

Figures \ref{fig:IS_plot3} and \ref{fig:IS_plot4} show the impact on the density and distribution of the examples for the new data sets obtained with Importance Sampling strategy.
<<IS_plot3, fig.cap="Relevance function and density of the target variable in the original and new data sets using Importance Sampling strategy.", echo=FALSE>>=
par(mar = c(5,5,2,5))
plot(density(ImbR$Tgt), xlab=expression(Tgt), main="")
lines(density(ISD$Tgt), col=3)
lines(density(IS1$Tgt), col=4)
lines(density(IS2$Tgt), col=6)

names <- c("ImbR", "ISD", "IS1", "IS2")
y <- sort(ImbR$Tgt)
pc <- phi.control(y, method="extremes")
y.relev <- phi(y,pc)

par(new=TRUE)
plot(y,y.relev, lty=2, col="grey", lwd=1, axes=F, ylab=NA, xlab=NA,type="l")
axis(4, xaxt="n",col ="grey", col.axis ="grey" , lwd = 1)
mtext(expression(phi(y)),4, col="grey", line=3)
legend(20,0.9, c(names,expression(phi())), col=c(1,3,4,6,"grey"), lty=c(1,1,1,1,2), lwd=c(1,1,1,1,1), bty="n", text.col=c(1,1,1,1,"grey"))
@



<<IS_plot4,fig.cap="Impact of Importance Sampling strategy in a binarized version of the data sets with the value 19 as the threshold between rare and normal cases.",out.width="0.8\\textwidth", echo=FALSE>>=
par(mfrow = c(2, 2))
plot(ImbR$X1, ImbR$X2, col= ifelse(ImbR$Tgt>19,2,3), pch=ifelse(ImbR$Tgt>19,"+", "-"), main="Original data",, xlim=c(-1,22), ylim=c(-1,22), xlab="X1", ylab="X2")
plot(jitter(ISD$X1), jitter(ISD$X2), col= ifelse(ISD$Tgt>19,2,3), pch=ifelse(ISD$Tgt>19,"+", "-"), main="U=0.5 O=0.5", xlim=c(-1,22), ylim=c(-1,22), xlab="X1", ylab="X2")
par(xpd=TRUE)
legend("topright", inset=c(0,0),col=c(2,3), c("rare cases", "normal cases"),pch=c("+", "-"), bty="n")
plot(jitter(IS1$X1), jitter(IS1$X2), col= ifelse(IS1$Tgt>19,2,3),pch=ifelse(IS1$Tgt>19,"+", "-"), main="U=0.9 O=0.2", xlim=c(-1,22), ylim=c(-1,22), xlab="X1", ylab="X2")
plot(jitter(IS2$X1), jitter(IS2$X2), col= ifelse(IS2$Tgt>19,2,3), pch=ifelse(IS2$Tgt>19,"+", "-"), main="U=0.5 O=0.8", xlim=c(-1,22), ylim=c(-1,22), xlab="X1", ylab="X2")

@


<<IS_new_plot4,fig.cap="Target variable distribution on ImbR and data sets changed through Importance Sampling strategy.", fig.height=6.5, fig.width=8, echo=FALSE, warning=FALSE>>=

g1 <- ggplot(ImbR, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = ImbR, aes(colour=Tgt)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("Original ImbR data")

g2 <- ggplot(ISD, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = ISD, aes(colour=Tgt), position=position_jitter(width=.5, height=.5)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("ISD data")
  
g3 <- ggplot(IS1, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = IS1, aes(colour=Tgt), position=position_jitter(width=.5, height=.5)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("IS1 data")

g4 <- ggplot(IS2, aes(x = X1, y = X2)) +  ylim(-0.5,20.5) + xlim(-0.5,20.5) +
  geom_point(data = IS2, aes(colour=Tgt), position=position_jitter(width=.5, height=.5)) +
  scale_color_gradient(low = "lightgreen", high="blue",limits=c(9.5,23.5)) +
  ggtitle("IS2 data")

plots <- list(g1, g2, g3, g4, ncol=2)

do.call(grid.arrange, plots)

@


% ====================================================================
\section{Distance Functions}\label{sec:distFunc}

In this section we briefly explain the different distance functions implemented, which can be used for calculating the neighbors of the examples along several strategies for classification or regression tasks.
The implementation of these functions was motivated by the inclusion in \UBL of several methods which depend on the nearest neighbors computation. Although several efficient tools exist for evaluating the nearest neighbors, they are mostly limited to the use of the Euclidean distance. In this context, restricting the user to the use of the Euclidean distance can be a limitation, namely because several data sets include nominal features which can and should also be considered in the neighbors computation. In fact, all the features contained in the data set, whether nominal or numeric, should be taken into account when computing the nearest neighbors. Thus, in order to avoid the restriction of computing nearest neighbors based only on the data set numeric features we have implemented several possible measures which can be used for data sets containing only nominal or numeric features or simultaneously both types. By the implementation of several distance functions, we aim at providing an increased flexibility for computing the nearest neighbors while ensuring that no feature information is wasted.

Several distance measures exist which can deal only with numeric or nominal features or can integrate both types in the distance evaluation. Distance functions such as \texttt{Canberra}, \texttt{Euclidean} or \texttt{Chebyshev} are able to deal solely with numeric attributes while the \texttt{Overlap} measure handles only nominal features. Other measures such as \texttt{HEOM} or \texttt{HVDM} try to use both types of features.

We now briefly describe the distance functions implemented in this package. We begin with the distance functions suitable for data sets with only numeric features. Let us suppose $x$ and $y$ are two examples of a data set with m features. The well-known Euclidean distance can be computed as shown in Equation \ref{eq:Eucl}. The Manhattan distance, also known as city-block distance or taxicab metric, may be calculated with Equation \ref{eq:Manhat}. 

\begin{equation}\label{eq:Eucl}
D(x,y)=\sqrt{ \sum_{i=1}^{m}(x_i-y_i)^2 }
\end{equation}

\begin{equation}\label{eq:Manhat}
D(x,y)=\sum_{i=1}^{m}|x_i-y_i|
\end{equation}
A generalization of these distance functions is obtained with the Minkowsky distance (cf. Equation \ref{eq:Minkowsky}). In this case, by setting $r$ to 1 or 2 we can obtain respectively the Manhattan and Euclidean distance functions.
\begin{equation}\label{eq:Minkowsky}
D(x,y)=\left( \sum_{i=1}^{m}|x_i-y_i|^r\right) ^{\frac{1}{r}}
\end{equation}

The Canberra distance, defined in Equation \ref{eq:Canberra}, and the Chebyshev distance (Equation \ref{eq:Chebychev}) are also functions which can be applied to evaluate the distance between examples described only by numeric features.



\begin{equation}\label{eq:Canberra}
D(x,y)= \sum_{i=1}^{m}\frac{|x_i-y_i|}{|x_i|+|y_i|}
\end{equation}

\begin{equation}\label{eq:Chebychev}
D(x,y)=\max_{i=1}^{m}|x_i-y_i|
\end{equation}

All the previous distance functions can be used in \UBL for computing the nearest neighbors. After selecting an appropriate approach to apply on a data set, it is only necessary to set the parameter \texttt{dist} of the approach to the desired distance function and the \texttt{p} parameter if it is a Minkowsky distance. We illustrate this in the next example.


<<numeric_measures_ex>>=
dat <- iris[-c(91:125),]
# using the default of smote to invert the frequencies of the data set
set.seed(123)
sm.Eu <- SmoteClassif(Species~., dat, dist="Euclidean", 
                      C.perc="extreme", k=3)
set.seed(123)
sm.Man1 <- SmoteClassif(Species~., dat, dist="Manhattan",
                        C.perc="extreme", k=3)
set.seed(123)
sm.Man2 <- SmoteClassif(Species~., dat, dist="p-norm", p=1,
                        C.perc="extreme", k=3)
set.seed(123)
sm.5norm <- SmoteClassif(Species~., dat, dist="p-norm", p=5, 
                         C.perc="extreme", k=3)
set.seed(123)
sm.Cheb <- SmoteClassif(Species~., dat, dist="Chebyshev", 
                        C.perc="extreme", k=3)
set.seed(123)
sm.Canb <- SmoteClassif(Species~., dat, dist="Canberra", 
                        C.perc="extreme", k=3)
@

The impact of using these distance functions with smote strategy can be visualized in Figure \ref{fig:dist_num}.


<<dist_num, fig.cap="Impact of using different distance functions with smote strategy.", echo=FALSE, fig.width=8, fig.height=7>>=
par(mfrow=c(3,2))
plot(dat[,1], dat[,2], pch=as.integer(dat[,5]), col=as.integer(dat[,5]), xlim=c(4,8), ylim=c(2,4.5), main="Original data")
plot(sm.Eu[,1], sm.Eu[,2], pch=as.integer(sm.Eu[,5]), col=as.integer(sm.Eu[,5]), xlim=c(4,8), ylim=c(2,4.5), main="Euclidean distance")
plot(sm.Man1[,1], sm.Man1[,2], pch=as.integer(sm.Man1[,5]), col=as.integer(sm.Man1[,5]), xlim=c(4,8), ylim=c(2,4.5), main="Manhattan distance")
plot(sm.5norm[,1], sm.5norm[,2], pch=as.integer(sm.5norm[,5]), col=as.integer(sm.5norm[,5]), xlim=c(4,8), ylim=c(2,4.5), main="Minkowsky with r=5")
plot(sm.Cheb[,1], sm.Cheb[,2], pch=as.integer(sm.Cheb[,5]), col=as.integer(sm.Cheb[,5]), xlim=c(4,8), ylim=c(2,4.5), main="Chebyshev distance")
plot(sm.Canb[,1], sm.Canb[,2], pch=as.integer(sm.Canb[,5]), col=as.integer(sm.Canb[,5]), xlim=c(4,8), ylim=c(2,4.5), main="Canberra distance")


@

All the previously described metrics do not perform any type of normalization. This step, if wanted, should be performed previously by the user.


Regarding nominal attributes, a distance function which is suitable for handling this type of variables is the overlap measure, which is defined in Equation \ref{eq:overlap}.


\begin{equation}\label{eq:overlap}
overlap(x,y) = \begin{cases} 1 &\mbox{if } x \neq y \\
0 & \mbox{if } x = y. \end{cases} 
\end{equation}

This distance function can be used in strategies that require the computation of nearest neighbors as follows:

<<overlap_ex>>=
# build a data set with all nominal features
library(DMwR)
data(algae)
clean.algae <- algae[complete.cases(algae),1:3]

# speed is considered the target class
summary(clean.algae)
ndat1 <- ENNClassif(speed~., clean.algae, dist="Overlap",  Cl=c("high", "medium"))
ndat2 <- ENNClassif(speed~., clean.algae, dist="Overlap",  Cl="all")

#all the smaller classes are the most important
ndat3 <- NCLClassif(speed~., clean.algae, dist="Overlap",  Cl="smaller")
# the most important classes are "high" and "low"
ndat4 <- NCLClassif(speed~., clean.algae, dist="Overlap",  Cl=c("high", "low"))

ndat5 <- SmoteClassif(speed~., clean.algae, dist="Overlap", C.perc="balance")
@

Figure \ref{fig:dist_overlap} shows the impact of using the overlap distance function, with several different strategies, on a data set consisting of only nominal variables.
<<dist_overlap, fig.cap="Using Overlap distance function with different strategies on a data set with only nominal features.", echo=FALSE>>=
g1 <- ggplot(clean.algae, aes(x = speed, fill=size))+ geom_bar()+ggtitle("Original data")

g2 <- ggplot(ndat2[[1]], aes(x = speed, fill=size))+ geom_bar()+ggtitle("ENN strategy")

g3 <- ggplot(ndat3, aes(x = speed, fill=size))+ geom_bar()+ggtitle("NCL strategy")

g4 <- ggplot(ndat5, aes(x = speed, fill=size))+ geom_bar()+ggtitle("smote strategy")

plots <- list(g1, g2, g3, g4)

do.call(grid.arrange, plots)
@

To evaluate the distance between examples described by nominal and numeric variables a simple adaptation of the previous distance functions can be performed. The Heterogeneous Euclidean-Overlap Metric function (HEOM) is a popular solution for these situations. Equations \ref{eq:HEOM} and \ref{eq:auxHEOM} describe how this distance is computed.


\begin{equation}\label{eq:HEOM}
HEOM(x,y)= \sqrt{\sum_{a=1}^{m}d_a^2(x_a,y_a) }
\end{equation}


\begin{equation}\label{eq:auxHEOM}
\mbox{where  } d_a(x,y)= \begin{cases} 1 & \mbox{if } x \vee y \mbox{ are unknown, else} \\
overlap(x,y) & \mbox{if } a \mbox{ is nominal, else} \\
\frac{|x-y|}{range_a}
\end{cases}
\end{equation}
\noindent where $range_a=max_a-min_a$


<<HEOM_ex>>=
# build a data set with nominal and numeric features
library(DMwR)
data(algae)
clean.algae <- algae[complete.cases(algae),1:5]

# speed is the target class
summary(clean.algae)
enn <- ENNClassif(speed~., clean.algae, dist="HEOM",  Cl="all", k=5)[[1]]
#consider all the smaller classes as the most important
ncl <- NCLClassif(speed~., clean.algae, dist="HEOM",  Cl="smaller")
sm <- SmoteClassif(speed~., clean.algae, dist="HEOM", C.perc="balance")
@


In Figure \ref{fig:dist_heom} we can observe the impact of using the HEOM distance function with several strategies.
<<dist_heom, fig.cap="Using HEOM distance function with different strategies on a data set with both nominal and numeric features.", echo=FALSE>>=
par(mfrow = c(2, 2))
plot(clean.algae$mxPH, clean.algae$mnO2,pch=as.integer(clean.algae$speed), col=as.integer(clean.algae$speed), xlim=c(7,9.5), ylim=c(1.5,13.5), main="original data")
plot(enn$mxPH, enn$mnO2,pch=as.integer(enn$speed), col=as.integer(enn$speed), xlim=c(7,9.5), ylim=c(1.5,13.5), main="ENN strategy applied")
plot(ncl$mxPH, ncl$mnO2,pch=as.integer(ncl$speed), col=as.integer(ncl$speed), xlim=c(7,9.5), ylim=c(1.5,13.5), main="NCL strategy applied")
plot(sm$mxPH, sm$mnO2,pch=as.integer(sm$speed), col=as.integer(sm$speed), xlim=c(7,9.5), ylim=c(1.5,13.5), main="smote strategy applied")
@


Other proposals, such as the Heterogeneous Value Difference Metric (HVDM), were tested for handling both nominal and numeric features. The HVDM uses the notion of Value Distance Metric (VDM) which was introduced by \cite{stanfill1986toward} to address the distance computation with nominal variables. The VDM metric is described in Equation \ref{eq:VDM}.

\begin{equation}\label{eq:VDM}
VDM_a(x,y)= \sum_{c=1}^{C}  \left\lvert \frac{N_{a,x,c}}{N_{a,x}}-\frac{N_{a,y,c}}{N_{a,y}}\right\rvert ^q
\end{equation}
where, \\
$a$ is the nominal attribute under consideration;\\
$C$ is the number of classes existing on the data set;\\
$q$ is a constant;\\
$N_{a,x,c}$ represents the number of examples which have value $x$ for the feature $a$ and class label $c$;\\
$N_{a,x}$ is the number of examples that have value $x$ for the feature $a$.\\


The HVDM distance function was proposed by \cite{wilson1997improved} and its definition, presented in Equations \ref{eq:HVDM}and \ref{eq:auxHVDM}, is similar to the HEOM.

\begin{equation}\label{eq:HVDM}
HVDM(x,y)= \sqrt{\sum_{a=1}^{m}d_a^2(x_a,y_a) }
\end{equation}

\begin{equation}\label{eq:auxHVDM}
\mbox{where  } d_a(x,y)= \begin{cases} 1 & \mbox{if } x \vee y \mbox{ are unknown, otherwise} \\
norm-vdm_a(x,y) & \mbox{if } a \mbox{ is nominal} \\
norm-diff_a(x,y) & \mbox{if } a \mbox{ is numeric} \\
\end{cases}
\end{equation}

The HVDM distance function uses a normalized version of the absolute value of the difference between two examples for the numeric attributes (Equation \ref{eq:HVDMnum}) and uses for the nominal attributes an also normalized version of the VDM measure for the nominal attributes (Equation \ref{eq:HVDMnom}) .

\begin{equation}\label{eq:HVDMnom}
norm-vdm_a(x,y)=\sqrt{VDM_a(x,y)}=\sqrt{\sum_{c=1}^{C}  \left\lvert \frac{N_{a,x,c}}{N_{a,x}}-\frac{N_{a,y,c}}{N_{a,y}}\right\rvert ^2}
\end{equation}


\begin{equation}\label{eq:HVDMnum}
norm-diff_a(x,y)= \frac{|x-y|}{4\sigma_a}
\end{equation}


Regarding Equation \ref{eq:HVDMnom}, several normalization of the VDM measure were proposed and tested in \cite{wilson1997improved}. The version presented here and implemented in \UBL was the one that achieved the best performance. We also highlight that the distance function proposed for the numeric attributes uses a different normalization which relies on the standard deviation of of each attribute $\sigma_a$.


The HVDM distance can be used simply by setting the \texttt{dist} parameter to ``HVDM". Although it is a function suitable for both nominal and numeric, if the data set provided contains only one type of attributes only the corresponding distance will be used.



<<HVDM_ex>>=
# build a data set with both nominal and numeric features
library(DMwR)
data(algae)
clean.algae <- algae[complete.cases(algae),c(1:6)]

# speed is considered the target class
summary(clean.algae)

dat1 <- SmoteClassif(speed~., clean.algae, dist="HVDM", C.perc="extreme")

dat2 <- NCLClassif(speed~., clean.algae, k=3, dist="HVDM", Cl="smaller")

dat3 <- TomekClassif(speed~., clean.algae, dist="HVDM", Cl="all", rem="both")
@

Figure \ref{fig:dist_HVDM} shows the result of applying HVDM distance function for several different strategies, on a data set consisting of numeric and nominal features.
<<dist_HVDM, fig.cap="Using HVDM distance function with different strategies.", echo=FALSE, fig.height=7, fig.width=8>>=
g1 <- ggplot(clean.algae, aes(x = mxPH, y=mnO2, col=speed))+ geom_point()+ggtitle("Original data")

g2 <- ggplot(dat1, aes(x = mxPH, y=mnO2, col=speed))+ geom_point()+ggtitle("smote'd data")

g3 <- ggplot(dat2, aes(x = mxPH, y=mnO2, col=speed))+ geom_point()+ggtitle("NCL strategy")

g4 <- ggplot(dat3[[1]], aes(x = mxPH, y=mnO2, col=speed))+ geom_point()+ggtitle("Tomek links strategy")

plots <- list(g1, g2, g3, g4)

do.call(grid.arrange, plots)
@


In Figure \ref{fig:dist_HVDM2} the impact of smote strategy applied with different distance functions on a data set can be observed.

<<dist_HVDM2, fig.cap="Using different distance functions with  smote strategy.", echo=FALSE>>=
set.seed(123)
d1 <- SmoteClassif(speed~., clean.algae, dist="HVDM", C.perc="extreme")
set.seed(123)
d2 <- SmoteClassif(speed~., clean.algae, dist="HEOM", C.perc="extreme")

# in order to use metrics for numeric attributes only we must remove the nominal attributes
set.seed(123)
d3 <- SmoteClassif(speed~., clean.algae[,3:6], dist="Euclidean", C.perc="extreme")
set.seed(123)
d4 <- SmoteClassif(speed~., clean.algae[,3:6], dist="Chebyshev", C.perc="extreme")
set.seed(123)
d5 <- SmoteClassif(speed~., clean.algae[,3:6], dist="Manhattan", C.perc="extreme")


g1 <- ggplot(clean.algae, aes(x = mxPH, y=mnO2, col=speed))+ geom_point()+ggtitle("Original data")

g2 <- ggplot(d1, aes(x = mxPH, y=mnO2, col=speed))+ geom_point()+ggtitle("HVDM distance")

g3 <- ggplot(d2, aes(x = mxPH, y=mnO2, col=speed))+ geom_point()+ggtitle("HEOM distance")

g4 <- ggplot(d3, aes(x = mxPH, y=mnO2, col=speed))+ geom_point()+ggtitle("Euclidean")

g5 <- ggplot(d4, aes(x = mxPH, y=mnO2, col=speed))+ geom_point()+ggtitle("Chebyshev distance")

g6 <- ggplot(d5, aes(x = mxPH, y=mnO2, col=speed))+ geom_point()+ggtitle("Manhattan distance")


plots <- list(g1, g2, g3, g4, g5, g6)

do.call(grid.arrange, plots)
@



% =======================================================
\section{Conclusions}\label{sec:conc}

We have presented \pUBL that aims at dealing with utility-based predictive tasks. This package offers several methods for multiclass and regression problems. The approaches implemented are pre-processing methods for changing the target variable distribution. This change in the data set is performed with the goal of incorporating the user preference bias. The use of pre-processing methods that change the original data set force the learning algorithms to focus on the most relevant cases for the user.

The existing strategies for dealing with utility-based problems as a pre-processing step present the advantage of allowing the use of any standard learning algorithm without changing it. Moreover, these methods do not compromise the interpretability of the models used. As possible disadvantages we must point the difficulty of determining the ideal distribution of the domain. In fact, a perfectly balanced distribution of examples is not always the solution that provides the best results.

\UBLp is a versatile tool for tackling problems at a pre-processing level that have some information regarding the domain. This package extends some methods previously developed for binary classification to a multiclass setting and also allows to deal with regression problems with multiple important regions. It offers to the user the possibility of manually defining how the data set should be changed for a selected pre-processing approach. Moreover, for each implemented approach it also enables the use of automatic methods for estimating the changes to apply. These automatic methods use the original domain disribution for assigning more importance to the least represented examples, a commom setting when learning from imbalanced domains.

\newpage

\bibliographystyle{alpha}
\bibliography{UBL}
\end{document}
